[{"title":"互联网金融行业数仓分层","date":"2019-01-13T03:34:12.678Z","path":"2019/01/13/互联网金融行业数仓分层/","text":"互联网金融行业数仓分层专业术语 ODL层 （Operational Data Layer）：操作数据层 外部数据什么样，该层数据就是什么样（关系型数据库、JSON格式等)部分关系型数据可以直接转IDL层 BDL层 （Base Data Layer）：基础数据层 ODL层经过简单格式化解析后存储到BDL层，常见于JSON日志格式的解析。 IDL层 （Interface Data Layer）：接口层，也称主题表，宽表 由BDL层经过去重、去噪、字典翻译、空值转化，日期格式化、关联JOIN、维度分析等清洗后的数据。如：用户、产品、绑卡、订单、用户行为等明细数据。 ADL层（Application Data Layer）：应用层 ，也称数据集市 通常与需求对接，由IDL层基于某些维度的深度加工统计汇总等操作转化而来，涉及到多个主题以及tmp数据之间的关联JOIN后的结果。 DIC层（Dictionary Data Layer）：字典层 存储一些诸如省、市、县区域表、渠道列表、商品类目等等表数据，可以从数据源直接sqoop生成dic_xxx表，也可以通过odl层转化层dic_表。 TMP层（Temporary Data Layer）：临时层 存储一些中间计算结果 简要说明: 层次间的转换没必要循规蹈矩，按部就班，适当做到灵活，避免重复清洗浪费资源 ODL层干净的关系型数据可以直接转换为IDL层数据，减少计算量 ODL层侧重与外部对接，BDL层/TMP层/IDL层侧重清洗，IDL层和ADL层侧重对外提供应用服务 层数太少不够灵活，太多则在数据推翻重洗耗时，时间成本（一个坑）数据源提供的数据越详细越好，避免后期大量重复的清洗工作。 “星型模型”和“雪花模型”简单解释： （1）星型模型：事实表+维度表（区域、类目、性别…)等多表通过预先JOIN冗余到一张宽表里去，常见IDL层。 （2）雪花模型：在计算的时候，才将事实表跟维度表做join。 现在一般都是采用（1）的模式，为什么呢？ 预先计算，挺高性能，避免后续重复计算。CPU和内存的资源永远比磁盘空间宝贵的多。至于（2)的方式，有点就是灵活，不需要太多的重复清洗，但是性能不如（1）. 建设思路 从需求出发，逆推应用层ADL结构，进而推导出它涉及的主题表IDL表结构，再推导可能涉及的基础表BDL表结构，最后分析所需的数据源取自何处。需求包含“明确”需求和“潜在”需求。 开发步骤 创建ODL、BDL、IDL、ADL层表结构(HQL) 确定数据抽取方案（增量或全量） 编写sqoop脚本将data同步到ODL层 编写ODL-&gt;BDL-&gt;IDL-&gt;ADL层ETL清洗脚本(HQL),注意：清洗的顺序，时间确保上一层的数据稳定，减少对下一层的影响 编写Hue workflow Ooize脚本 打通Kylin、FineBI、Hive关系，实现数据可视化、可导出目标,将稳定后所有脚本WIKI上保存一份 其他相关的请参照原博客 作者：水星有鱼链接：https://www.jianshu.com/p/f941967aeee8","tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://wxzhongwang.github.io/tags/数据仓库/"}]},{"title":"SparkStreaming和Storm","date":"2019-01-13T03:33:19.819Z","path":"2019/01/13/SparkStreaming和Storm的区别/","text":"SparkStreaming和StormStorm和Spark Streaming都是分布式流处理的开源框架，但是它们之间还是有一些区别的，这里将进行比较并指出它们的重要的区别。 处理模型以及延迟虽然这两个框架都提供可扩展性(Scalability)和可容错性(Fault Tolerance),但是它们的处理模型从根本上说是不一样的。Storm处理的是每次传入的一个事件，而Spark Streaming是处理某个时间段窗口内的事件流。因此，Storm处理一个事件可以达到亚秒级的延迟，而Spark Streaming则有秒级的延迟。 容错和数据保证在容错数据保证方面的权衡方面，Spark Streaming提供了更好的支持容错状态计算。在Storm中，当每条单独的记录通过系统时必须被跟踪，所以Storm能够至少保证每条记录将被处理一次，但是在从错误中恢复过来时候允许出现重复记录，这意味着可变状态可能不正确地被更新两次。而Spark Streaming只需要在批处理级别对记录进行跟踪处理，因此可以有效地保证每条记录将完全被处理一次，即便一个节点发生故障。虽然Storm的 Trident library库也提供了完全一次处理的功能。但是它依赖于事务更新状态，而这个过程是很慢的，并且通常必须由用户实现。 简而言之,如果你需要亚秒级的延迟，Storm是一个不错的选择，而且没有数据丢失。如果你需要有状态的计算，而且要完全保证每个事件只被处理一次，Spark Streaming则更好。Spark Streaming编程逻辑也可能更容易，因为它类似于批处理程序，特别是在你使用批次(尽管是很小的)时。 实现和编程APIStorm主要是由Clojure语言实现，SparkStreaming是由Scala实现。如果你想看看这两个框架是如何实现的或者你想自定义一些东西你就得记住这一点。Storm是由BackType和Twitter开发，而Spark Streaming是在UC Berkeley开发的。 Storm提供了Java API，同时也支持其他语言的API。SparkStreaming支持Scala和Java语言(其实也支持Python)。另外SparkStreaming的一个很棒的特性就是它是在Spark框架上运行的。这样你就可以想使用其他批处理代码一样来写SparkStreaming程序，或者是在Spark中交互查询。这就减少了单独编写流批量处理程序和历史数据处理程序。 生产支持Storm已经出现好多年了，而且自从2011年开始就在Twitter内部生产环境中使用，还有其他一些公司。而Spark Streaming是一个新的项目，并且在2013年仅仅被Sharethrough使用(据作者了解)。 Storm是 Hortonworks Hadoop数据平台中流处理的解决方案，而Spark Streaming出现在 MapR的分布式平台和Cloudera的企业数据平台中。除此之外，Databricks是为Spark提供技术支持的公司，包括了Spark Streaming。 集群管理集成尽管两个系统都运行在它们自己的集群上，Storm也能运行在Mesos，而SparkStreaming能运行在YARN 和 Mesos上。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"Hadoop Hive Hbase 简单区别及应用场景","date":"2019-01-13T03:30:12.445Z","path":"2019/01/13/Hadoop Hive Hbase 简单区别及应用场景/","text":"Hadoop Hive Hbase 简单区别及应用场景Hadoop它是一个分布式计算+分布式文件系统，前者其实就是MapReduce，后者是HDFS。后者可以独立运行，前者可以选择性使用，也可以不使用。 Hive通俗的说是一个数据仓库，仓库中的数据是被HDFS管理的数据文件，它支持类似sql语句的功能，你可以通过该语句完成分布式环境下的计算功能，Hive会把语句转换成MapReduce，然后交给Hadoop执行。这里的计算，仅限于查找和分析，而不是更新、增加和删除。它的优势是对历史数据进行处理，用时下流行的说法是离线计算，因为它的底层是MapReduce，MapReduce在实时计算上性能很差。它的做法是把数据文件加载进来作为一个Hive表（或者外部表），让你觉得你的sql操作的是传统的表。 HBase通俗的说，HBase的作用类似于数据库，传统数据库管理的是集中的本地数据文件，而HBase基于Hdfs实现对分布式数据文件的管理，比如增删改查。也就是说，HBase只是利用Hadoop的Hdfs帮助其管理数据的持久化文件（HFile），它跟MapReduce没任何关系。HBase的优势在于实时计算，所有实时数据都直接存入Hbase中，客户端通过API直接访问Hbase，实现实时计算。由于它使用的是nosql，或者说是列式结构，从而提高了查找性能，使其能运用于大数据场景，这是它跟MapReduce的区别。 总结： Hadoop是Hive和HBase的基础，Hive依赖Hadoop HBase仅依赖Hadoop的Hdfs模块。 Hive适用于离线数据的分析，操作的是通用格式的（如通用的日志文件）、被Hadoop管理的数据文件，它支持类sql，比编写MapReduce的java代码来的更加方便，它的定位是数据仓库，存储和分析历史数据 Hbase适用于实时计算，采用列式结构的nosql，操作的是自己生成的特殊格式的HFile、被hadoop管理的数据文件，它的定位是数据库，或者叫DBMS 最后补充一下：Hive可以直接操作Hdfs中的文件作为它的表的数据，也可以使HBase数据库作为它的表","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"Hadoop概念","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/Hadoop/","text":"Hadoop是一个开源的框架，可编写和运行分布式应用处理大规模数据，是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式。 Hadoop概念 Hadoop是一个开源的框架，可编写和运行分布式应用处理大规模数据，是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式。 ==不是为了大数据而大数据== Hadoop 是以一种可靠、高效、可伸缩的方式进行处理的。Hadoop 是可靠的，因为它假设计算元素和存储会失败，因此它维护多个工作数据副本，确保能够针对失败的节点重新分布处理。Hadoop 是高效的，因为它以并行的方式工作，通过并行处理加快处理速度。Hadoop 还是可伸缩的，能够处理 PB 级数据。核心Hadoop的核心就是==HDFS==和==MapReduce===，Hadoop旗下有很多经典子项目，比如HBase、Hive等，这些都是基于HDFS和MapReduce发展出来的。要想了解Hadoop，就必须知道HDFS和MapReduce是什么。HDFSHadoop Distributed File System，Hadoop 分布式文件系统高度容错性的系统，适合部署在廉价的机器上，HDFS能提供高吞吐量的数据访问，适合那些有着超大数据集（large data set）的应用程序。MapReduceMapreduce是一个计算框架，一个处理分布式海量数据的软件框架及计算集群。 用处 搜索引擎 - 设计Hadoop的初衷，为了针对大规模的网页快速建立索引） 大数据存储 - 利用Hadoop的分布式存储能力，例如数据备份、数据仓库等。 大数据处理 - 利用Hadoop的分布式处理能力，例如数据挖掘、数据分析等。 科学研究 - Hadoop是一种分布式的开源框架，对于分布式计算有很大程度地参考价值。 优缺点优点==高可靠性==Hadoop按位存储和处理数据的能力值得信赖。 ==高扩展性==Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。 ==高效性==Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。 ==高容错性==Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。 ==低成本==与一体机、商用数据仓库以及QlikView、Yonghong Z-Suite等数据集市相比，hadoop是开源的，项目的软件成本因此会大大降低。 Hadoop设计对硬件需求比较低，只须运行在低廉的商用硬件集群上，而无需昂贵的高可用性机器上。廉价的商用机也就意味着大型集群中出现节点故障情况的概率非常高。这就要求设计HDFS时要充分考虑数据的可靠性，安全性及高可用性。 缺点==不适合低延迟数据访问== 如果要处理一些用户要求时间比较短的低延迟应用请求，则HDFS不适合。HDFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的，这就可能要求以高延迟作为代价。 改进策略：对于那些有低延时要求的应用程序，HBase是一个更好的选择。通过上层数据管理项目来尽可能地弥补这个不足。在性能上有了很大的提升，它的口号就是goes real time。使用缓存或多master设计可以降低client的数据请求压力，以减少延时。还有就是对HDFS系统内部的修改，这就得权衡大吞吐量与低延时了，HDFS不是万能的银弹。 ==无法高效存储大量小文件== 因为Namenode把文件系统的元数据放置在内存中，所以文件系统所能容纳的文件数目是由Namenode的内存大小来决定。一般来说，每一个文件、文件夹和Block需要占据150字节左右的空间，所以，如果你有100万个文件，每一个占据一个Block，你就至少需要300MB内存。当前来说，数百万的文件还是可行的，当扩展到数十亿时，对于当前的硬件水平来说就没法实现了。还有一个问题就是，因为Maptask的数量是由splits来决定的，所以用MR处理大量的小文件时，就会产生过多的Maptask，线程管理开销将会增加作业时间。举个例子，处理10000M的文件，若每个split为1M，那就会有10000个Maptasks，会有很大的线程开销；若每个split为100M，则只有100个Maptasks，每个Maptask将会有更多的事情做，而线程的管理开销将减小很多。 ==不支持多用户写入及任意修改文件== 在HDFS的一个文件中只有一个写入者，而且写操作只能在文件末尾完成，即只能执行追加操作。目前HDFS还不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"流处理、批处理、交互式查询","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/流处理、批处理、交互式查询/","text":"我们将大数据处理按处理时间的跨度要求分为以下几类 基于实时数据流的处理，通常的时间跨度在数百毫秒到数秒之间 基于历史数据的交互式查询，通常时间跨度在数十秒到数分钟之间 复杂的批量数据处理，通常的时间跨度在几分钟到数小时之间","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"大数据","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/大数据/","text":"大数据生命周期 基础设施层，涵盖计算资源、内存与存储和网络互联，具体表现为计算节点、集群、机柜和数据中心。 数据存储和管理层，包括文件系统、数据库和类似YARN的资源管理系统。 计算处理层，如hadoop、MapReduce和Spark 在此之上的各种不同计算范式，如批处理、流处理和图计算等，包括衍生出编程模型的计算模型，如BSP、GAS等 数据分析和可视化基于计算处理层。分析包括简单的查询分析、流分析以及更复杂的分析(如机器学习、图计算等)。查询分析多基于表结构和关系函数，流分析基于数据、事件流以及简单的统计分析，而复杂分析则基于更复杂的数据结构与方法，如图、矩阵、迭代计算和线性代数。一般意义的可视化是对分析结果的展示。但是通过交互式可视化，还可以探索性地提问，使分析获得新的线索，形成迭代的分析和可视化。基于大规模数据的实时交互可视化分析以及在这个过程中引入自动化的因素是目前研究的热点。 大数据技术生态大数据的基本处理流程与传统数据处理流程并无太大差异，主要区别在于：由于大数据要处理大量、非结构化的数据，所以在各处理环节中都可以采用并行处理。目前，Hadoop、MapReduce和Spark等分布式处理方式已经成为大数据处理各环节的通用处理方法。 大数据采集与预处理 存储层 预处理层 采集层 在大数据的生命周期中，数据采集处于第一个环节。根据MapReduce产生数据的应用系统分类，大数据的采集主要有4种来源：管理信息系统、Web信息系统、物理信息系统、科学实验系统。对于不同的数据集，可能存在不同的结构和模式，如文件、XML树、关系表等，表现为数据的异构性。对多个异构的数据集，需要做进一步集成处理或整合处理，将来自不同数据集的数据收集、整理、清洗、转换后，生成到一个新的数据集，为后续查询和分析处理提供统一的数据视图。针对管理信息系统中异构数据库集成技术、Web信息系统中的实体识别技术和DeepWeb集成技术、传感器网络数据融合技术已经有很多研究工作，取得了较大的进展，已经推出了多种数据清洗和质量控制工具。 大数据的存储和管理按数据类型的不同，大数据的存储和管理采用不同的技术路线，大致可以分为3类。 第1类主要面对的是大规模的结构化数据。针对这类大数据，通常采用新型数据库集群。它们通过列存储或行列混合存储以及粗粒度索引等技术，结合MPP(MassiveParallelProcessing)架构高效的分布式计算模式，实现对PB量级数据的存储和管理。这类集群具有高性能和高扩展性特点，在企业分析类应用领域已获得广泛应用; 第2类主要面对的是半结构化和非结构化数据。应对这类应用场景，基于Hadoop开源体系的系统平台更为擅长。它们通过对Hadoop生态体系的技术扩展和封装，实现对半结构化和非结构化数据的存储和管理; 第3类面对的是结构化和非结构化混合的大数据，因此采用MPP并行数据库集群与Hadoop集群的混合来实现对百PB量级、EB量级数据的存储和管理。一方面，用MPP来管理计算高质量的结构化数据，提供强大的SQL和OLTP型服务;另一方面，用Hadoop实现对半结构化和非结构化数据的处理，以支持诸如内容检索、深度挖掘与综合分析等新型应用。这类混合模式将是大数据存储和管理未来发展的趋势。","tags":[{"name":"大数据","slug":"大数据","permalink":"https://wxzhongwang.github.io/tags/大数据/"}]},{"title":"Hadoop技术体系","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/大数据相关技术（Hadoop体系）/","text":"Hadoop 里面包括几个组件HDFS、MapReduce、YARN和ZooKeeper等一系列技术，HDFS是存储数据的地方就像我们电脑的硬盘一样文件都存储在这个上面，MapReduce是对数据进行处理计算的，YARN是体现Hadoop平台概念的重要组件，有了它大数据生态体系的其它软件就能在hadoop上运行了，这样能更好的利用HDFS大存储的优势和节省更多的资源比如我们就不用再单独建一个spark的集群了，让它直接跑在现有的hadoop yarn上面就可以了。ZooKeeper本身是一个非常牢靠的记事本，用于记录一些概要信息。Hadoop依靠这个记事本来记录当前哪些节点正在用，哪些已掉线，哪些是备用等，以此来管理机群。 Hadoop技术体系HadoopHadoop 里面包括几个组件HDFS、MapReduce、YARN和ZooKeeper等一系列技术，HDFS是存储数据的地方就像我们电脑的硬盘一样文件都存储在这个上面，MapReduce是对数据进行处理计算的，YARN是体现Hadoop平台概念的重要组件，有了它大数据生态体系的其它软件就能在hadoop上运行了，这样能更好的利用HDFS大存储的优势和节省更多的资源比如我们就不用再单独建一个spark的集群了，让它直接跑在现有的hadoop yarn上面就可以了。ZooKeeper本身是一个非常牢靠的记事本，用于记录一些概要信息。Hadoop依靠这个记事本来记录当前哪些节点正在用，哪些已掉线，哪些是备用等，以此来管理机群。 HDFSHadoop Distributed File System，Hadoop 分布式文件系统高度容错性的系统，适合部署在廉价的机器上，HDFS能提供高吞吐量的数据访问，适合那些有着超大数据集（large data set）的应用程序。 MapReduceMapreduce是一个计算框架，一个处理分布式海量数据的软件框架及计算集群。 Map （映射） Reduce (简化)举个例子：假设你的手机通话信息保存在一个HDFS的文件callList.txt中，你想找到你与同事A的所有通话记录并排序。因为HDFS会把callLst.txt分成几块分别存，比如说5块，那么对应的Map过程就是找到这5块所在的5个节点，让它们分别找自己存的那块中关于同事A的通话记录，对应的Reduce过程就是把5个节点过滤后的通话记录合并在一块并按时间排序。MapReduce的计算模型通常把HDFS作为数据来源，很少会用到其它数据来源比如HBase。 Hbase这是Hadoop生态体系中的NOSQL数据库，他的数据是按照key和value的形式存储的并且key是唯一的，所以它能用来做数据的排重，它与MYSQL相比能存储的数据量大很多。所以他常被用于大数据处理完成之后的存储目的地。 HDFS和HBase是依靠外存（即硬盘）的分布式文件存储实现和分布式表存储实现。HDFS是一个分布式的“云存储”文件系统，它会把一个文件分块并分别保存，取用时分别再取出、合并。重要的是，这些分块通常会在3个节点（即集群内的服务器）上各有1个备份，因此即使出现少数节点的失效（如硬盘损坏、掉电等），文件也不会失效。如果说HDFS是文件级别的存储，那HBase则是表级别的存储。HBase是表模型，但比SQL数据库的表要简单的多，没有连接、聚集等功能。HBase的表是物理存储到HDFS的，比如把一个表分成4个HDFS文件并存储。由于HDFS级会做备份，所以HBase级不再备份。MapReduce则是一个计算模型，而不是存储模型；MapReduce通常与HDFS紧密配合。 HiveHive 是一种底层封装了Hadoop 的数据仓库处理工具，使用类SQL的HiveQL语言实现数据查询，所有Hive 的数据都存储在Hadoop 兼容的文件系统（如HDFS）中。Hive在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive设定的目录下，++因此，Hive不支持对数据的改写和添加，所有的数据都是在加载的时候确定的++。对于会SQL语法的来说就是神器，它能让你处理大数据变的很简单，不会再费劲的编写MapReduce程序。 Spark它是用来弥补基于MapReduce处理数据速度上的缺点，它的特点是把数据装载到内存中计算而不是去读慢的要死进化还特别慢的硬盘。特别适合做迭代运算，所以算法流们特别稀饭它。它是用scala编写的。Java语言或者Scala都可以操作它，因为它们都是用JVM的。 其他相关技术Sqoop这个是用于把Mysql里的数据导入到Hadoop里的。当然你也可以不用这个，直接把Mysql数据表导出成文件再放到HDFS上也是一样的，当然生产环境中使用要注意Mysql的压力。 Flume apache Flume 是一个从可以收集例如日志，事件等数据资源，并将这些数量庞大的数据从各项数据资源中集中起来存储的工具/服务，或者数集中机制。flume具有高可用，分布式，配置工具，其设计的原理也是基于数据流，如日志数据从各种网站服务器上汇集起来存储到HDFS，HBase等集中存储器中。 一般实时系统，所选用组件如下 数据采集 ：负责从各节点上实时采集数据，选用Flume来实现 数据接入 ：由于采集数据的速度和数据处理的速度不一定同步，因此添加一个消息中间件来作为缓冲，选用apache的kafka 流式计算 ：对采集到的数据进行实时分析，选用apache的storm 数据输出 ：对分析后的结果持久化，暂定用mysql，另一方面是模块化之后，假如当Storm挂掉了之后，数据采集和数据接入还是继续在跑着，数据不会丢失，storm起来之后可以继续进行流式计算； KafkaKafka的整体架构非常简单，是显式分布式架构，producer、broker（kafka）和consumer都可以有多个。Producer，consumer实现Kafka注册的接口，数据从producer发送到broker，broker承担一个中间缓存和分发的作用。broker分发注册到系统中的consumer。broker的作用类似于缓存，即活跃的数据和离线处理系统之间的缓存。客户端和服务器端的通信，是基于简单，高性能，且与编程语言无关的TCP协议。 Kafka是一种分布式的、基于发布/订阅的消息系统。在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算（KAFKA+STORM+REDIS）。 特点： 消息持久化：通过O(1)的磁盘数据结构提供数据的持久化 高吞吐量：每秒百万级的消息读写 分布式：扩展能力强 多客户端支持：java、php、python、c++ …… 实时性：生产者生产的message立即被消费者可见 Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实 现上完全不同，此外它并不是JMS规范的实现。 Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer 无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"消息队列","date":"2018-12-08T03:18:02.000Z","path":"2018/12/08/消息队列/","text":"一、消息队列的基本概念1.1 Broker==Broker== 的概念来自与Apache ActiveMQ，通俗的讲就是消息队列服务器。 1.2 消息生产者和消费者 消息生产者 ==Producer==：发送消息到消息队列。 消息消费者 ==Consumer==：从消息队列接收消息。 1.3 消息模型点对点消息队列模型消息生产者向一个特定的队列发送消息，消息消费者从该队列中接收消息。消息的生产者和消费者可以不同时处于运行状态。每一个成功处理的消息都由消息消费者签收确认（Acknowledge）。 发布订阅消息模型-Topic发布订阅消息模型中，支持向一个特定的主题Topic发布消息，0个或多个订阅者接收来自这个消息主题的消息。在这种模型下，发布者和订阅者彼此不知道对方。实际操作过程中，必须先订阅，再发送消息，而后接收订阅的消息，这个顺序必须保证。 1.4 消息顺序性保证基于Queue消息模型，利用FIFO先进先出的特性，可以保证消息的顺序性。 1.5 消息的ACK确认机制即消息的Ackownledge确认机制：为了保证消息不丢失，消息队列提供了消息Acknowledge机制，即ACK机制，当Consumer确认消息已经消费处理，发送一个ACK给消息队列，此时消息队列便可以删除这个消息了。如果Consumer宕机/关闭，没有发送ACK，消息队列将认为这个消息没有被处理，会将这个消息重新发送给其他的Consumer重新消费处理。 1.6 消息的持久化消息的持久化，对于一些关键的核心业务来说是非常重要的，启用消息持久化后，消息队列宕机重启后，消息可以从持久化存储恢复，消息不丢失，可以继续消费处理。 1.7 消息的同步和异步收发同步消息的收发支持同步收发的方式同步收发场景下，消息生产者和消费者双向应答模式，例如：张三写封信送到邮局中转站，然后李四从中转站获得信，然后在写一份回执信，放到中转站，然后张三去取，当然张三写信的时候就得写明回信地址。消息的接收如果以同步的方式(Pull)进行接收，如果队列中为空，此时接收将处于同步阻塞状态，会一直等待，直到消息的到达。 异步消息的收发同样支持异步方式：异步发送消息，不需要等待消息队列的接收确认。异步接收消息，以Push的方式触发消息消费者接收消息。 1.8 消息的事务支持消息的收发处理支持事务，例如：在任务中心场景中，一次处理可能涉及多个消息的接收、处理，这处于同一个事务范围内，如果一个消息处理失败，事务回滚，消息重新回到队列中。 二、JMS消费服务Java消息服务（Java Message Service，JMS）应用程序接口是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。 点对点与发布订阅最初是由JMS定义的。这两种模式主要区别或解决的问题就是发送到队列的消息能否重复消费(多订阅) 。 JMS规范目前支持两种消息模型： 点对点（point to point， queue） 发布/订阅（publish/subscribe，topic） 2.1 点对点：Queue，不可重复消费消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 P2P模式包含三个角色： 消息队列（Queue） 发送者(Sender) 接收者(Receiver) 每个消息都被发送到一个特定的队列，接收者从队列中获取消息。队列保留着消息，直到他们被消费或超时。 2.2 发布/订阅：Topic，可以重复消费消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。 支持订阅组的发布订阅模式：发布订阅模式下，当发布者消息量很大时，显然单个订阅者的处理能力是不足的。实际上现实场景中是多个订阅者节点组成一个订阅组负载均衡消费topic消息即分组订阅，这样订阅者很容易实现消费能力线性扩展。可以看成是一个topic下有多个Queue，每个Queue是点对点的方式，Queue之间是发布订阅方式。 2.3 区别点对点模式生产者发送一条消息到queue，一个queue可以有很多消费者，但是一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有一个可用的消费者，所以Queue实现了一个可靠的负载均衡。 发布订阅模式发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。topic实现了发布和订阅，当你发布一个消息，所有订阅这个topic的服务都能得到这个消息，所以从1到N个订阅者都能得到这个消息的拷贝。 三、流行模型对比传统企业型消息队列ActiveMQ遵循了JMS规范，实现了点对点和发布订阅模型，但其他流行的消息队列RabbitMQ、Kafka并没有遵循JMS规范。 3.1 RabbitMQRabbitMQ实现了AQMP协议，AQMP协议定义了消息路由规则和方式。生产端通过路由规则发送消息到不同queue，消费端根据queue名称消费消息。RabbitMQ既支持内存队列也支持持久化队列，消费端为推模型，消费状态和订阅关系由服务端负责维护，消息消费完后立即删除，不保留历史消息。 点对点生产端发送一条消息通过路由投递到Queue，只有一个消费者能消费到。 多订阅当RabbitMQ需要支持多订阅时，发布者发送的消息通过路由同时写到多个Queue，不同订阅组消费不同的Queue。所以支持多订阅时，消息会多个拷贝。 3.2 KafkaKafka只支持消息持久化，消费端为拉模型，消费状态和订阅关系由客户端端负责维护，消息消费完后不会立即删除，会保留历史消息。因此支持多订阅时，消息只会存储一份就可以了。但是可能产生重复消费的情况。","tags":[{"name":"MessageQueue","slug":"MessageQueue","permalink":"https://wxzhongwang.github.io/tags/MessageQueue/"}]},{"title":"你好，世界","date":"2018-08-31T09:54:54.000Z","path":"2018/08/31/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[{"name":"前端","slug":"前端","permalink":"https://wxzhongwang.github.io/tags/前端/"},{"name":"博客系统","slug":"博客系统","permalink":"https://wxzhongwang.github.io/tags/博客系统/"}]},{"title":"sass sass-loader","date":"2018-06-12T08:45:02.000Z","path":"2018/06/12/windows下安装node-sass和sass-loader失败/","text":"window下无法安装sass sass-loader node-sass安装失败的原因是网络限制导致无法下载.node文件 推荐方法：使用淘宝镜像 1npm set sass_binary_site=https://npm.taobao.org/mirrors/node-sass/ or 12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install node-sass sass-loader -S 其他翻墙、手动导入文件的方式不推荐。","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"}]},{"title":"Mock.js","date":"2018-04-08T05:22:02.000Z","path":"2018/04/08/Mockjs/","text":"目前的大部分公司的项目都是采用的前后端分离, 后端接口的开发和前端人员是同时进行的. 那么这个时候就会存在一个问题, 在页面需要使用大量数据进行渲染生成前, 后端开发人员的接口也许并没有写完, 作为前端的我们也就没有办法获取数据. 所以 前端工程师就需要自己按照接口文档模拟后端人员提供的数据, 以此进行页面的开发.这个时候, Mock.js的作用就体现出来了, 在数据量较大的情况下, 我们不用一个一个的编写数据, 只需要根据接口文档将数据的格式填入,Mock.js就能够自动的按需生成大量的模拟数据. 且Mock.js提供了大量的数据类型, 包括文本, 数字, 布尔值, 日期, 邮箱, 链接, 图片, 颜色等. Mock.jsMockjs是什么?目前的大部分公司的项目都是采用的前后端分离, 后端接口的开发和前端人员是同时进行的. 那么这个时候就会存在一个问题, 在页面需要使用大量数据进行渲染生成前, 后端开发人员的接口也许并没有写完, 作为前端的我们也就没有办法获取数据. 所以 前端工程师就需要自己按照接口文档模拟后端人员提供的数据, 以此进行页面的开发.这个时候, Mock.js的作用就体现出来了, 在数据量较大的情况下, 我们不用一个一个的编写数据, 只需要根据接口文档将数据的格式填入,Mock.js就能够自动的按需生成大量的模拟数据. 且Mock.js提供了大量的数据类型, 包括文本, 数字, 布尔值, 日期, 邮箱, 链接, 图片, 颜色等. 安装Mockjs123npm install mockjs -S or npm install mockjs -D 引用MockjsMock.js暴露了一个全局的Mock对象, 我们只需要将Mock对象引入到文件中, 调用Mock对象的方法即可 CommonJS的引入方式 12345678910//CommonJS引入let Mock = require('mockjs) //调用Mock.mock()方法模拟数据let data = Mock.mock(&#123;'list|1-10': [&#123; 'id|+1': 1&#125;]&#125;);console.log(data); ES6的引入方式 123456789//ES6的引入方式import Mock from 'mockjs' let data = Mock.mock(&#123;'list|1-10': [&#123; 'id|+1': 1&#125;]&#125;);console.log(data); 简单用法Mock对象提供了4个方法, 分别是 Mock.mock() Mock.setup() Mock.valid Mock.toJSONSchema() 以及一个工具库 Mock.Random. 其中我们经常使用到的就是Mock.mock()和Mock.Random.","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"}]},{"title":"Linux常用命令","date":"2018-03-06T08:18:02.000Z","path":"2018/03/06/Linux 常用命令/","text":"Linux常用命令 #Linux 常用命令 cd ==cd /== 进入主目录 ==cd ~== 进入Home目录 ==cd -== 进入上一次工作中路径 ls ==ls -a== 列出所有 ==ls -r== 反序排列 ==ls -t== 以文件修改时间排列 ==ls -l== 将文件名,文件所有者，文件大小信息详细信息列出来 pwd ==pwd== 展示当前工作目录路径 mkdir创建文件夹可用选项： ==mkdir -m==: 对新建目录设置存取权限,也可以用chmod命令设置; ==mkdir -p==: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后,系统将自动建立那 些尚不在的目录,即一次可以建立多个目录; rm删除文件，删除一个目录中的一个或多个文件或目录，如果没有使用- r选项，则rm不会删除目录 rmdir从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对其父目录的写权限。 mv移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。 cp将源文件复制至目标文件，或将多个源文件复制至目标目录。 free显示系统内存使用情况，包括物理内存、交互区内存(swap)和内核缓冲区内存 -b 以Byte显示内存使用情况 -k 以kb为单位显示内存使用情况 -m 以mb为单位显示内存使用情况 -g 以gb为单位显示内存使用情况 -s &lt;间隔秒数&gt; 持续显示内存 -t 显示内存使用总合 cat cat主要有三大功能： 一次显示整个文件:cat filename 从键盘创建一个文件:cat &gt; filename 只能创建新文件,不能编辑已有文件. 将几个文件合并为一个文件:cat file1 file2 &gt; file -b对非空输出行号 -n输出所有行号","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"HttpStatusCode","date":"2018-02-08T04:18:45.000Z","path":"2018/02/08/HttpStatusCode/","text":"HttpStatusCode12345678 /* 1xx：相关信息 2xx：操作成功 3xx：重定向 4xx：客户端错误 5xx：服务器错误*/ 字段 状态码 说明 Continue 100 指示客户端可能继续其请求。 SwitchingProtocols 100 指示正在更改协议版本或协议。 OK 200 指示请求成功，且请求的信息包含在响应中。 Created 201 指示请求导致在响应被发送前创建新资源。 Accepted 202 指示请求已被接受做进一步处理。 NonAuthoritativeInformation 202 指示返回的元信息来自缓存副本而不是原始服务器，因此可能不正确。 NoContent 204 指示请求成功，指示已成功处理请求并且响应已被设定为无内容。 ResetContent 205 指示客户端应重置（或重新加载）当前资源。 PartialContent 206 指示响应是包括字节范围的 GET 请求所请求的部分响应。 MultipleChoices 300 指示请求的信息有多种表示形式，默认操作是将此状态视为重定向，并遵循与此响应关联的 Location 头的内容。 Ambiguous 300 指示请求的信息有多种表示形式。默认操作是将此状态视为重定向，并遵循与此响应关联的 Location 头的内容。 MovedPermanently 301 指示请求的信息已移到 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。 Moved 301 指示请求的信息已移到 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求将使用 GET 方法。 Found 302 指示请求的信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求将使用 GET 方法。 Redirect 302 指示请求的信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求将使用 GET 方法。 SeeOther 303 作为 POST 的结果，SeeOther 将客户端自动重定向到 Location 头中指定的 URI。用 GET 生成对 Location 头所指定的资源的请求。 RedirectMethod 303 作为 POST 的结果，RedirectMethod 将客户端自动重定向到 Location 头中指定的 URI。用 GET 生成对 Location 头所指定的资源的请求。 NotModified 304 指示客户端的缓存副本是最新的。未传输此资源的内容。 UseProxy 305 指示请求应使用位于 Location 头中指定的 URI 的代理服务器。 Unused 306 是未完全指定的 HTTP/1.1 规范的建议扩展。 TemporaryRedirect 307 指示请求信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求还将使用 POST 方法。 RedirectKeepVerb 307 指示请求信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求还将使用 POST 方法。 BadRequest 400 指示服务器未能识别请求。如果没有其他适用的错误，或者如果不知道准确的错误或错误没有自己的错误代码，则发送 BadRequest。 Unauthorized 401 指示请求的资源要求身份验证。WWW-Authenticate头包含如何执行身份验证的详细信息。 PaymentRequired 402 保留 PaymentRequired 以供将来使用。 Forbidden 403 指示服务器拒绝满足请求。 NotFound 404 指示请求的资源不在服务器上。 MethodNotAllowed 405 指示请求的资源上不允许请求方法（POST 或 GET）。 NotAcceptable 406 指示客户端已用 Accept 头指示将不接受资源的任何可用表示形式。 ProxyAuthenticationRequired 407 指示请求的代理要求身份验证。Proxy-authenticate 头包含如何执行身份验证的详细信息。 RequestTimeout 408 指示客户端没有在服务器期望请求的时间内发送请求。 Conflict 409 指示由于服务器上的冲突而未能执行请求。 Gone 410 指示请求的资源不再可用。 LengthRequired 411 指示缺少必需的 Content-length 头。 PreconditionFailed 412 指示为此请求设置的条件失败，且无法执行此请求。条件是用条件请求标头（如 If-Match、If-None-Match 或 If-Unmodified-Since）设置的。 RequestEntityTooLarge 413 指示请求太大，服务器无法处理。 RequestUriTooLong 414 指示 URI 太长。 UnsupportedMediaType 415 指示请求是不支持的类型。 RequestedRangeNotSatisfiable 416 RequestedRangeNotSatisfiable指示无法返回从资源请求的数据范围，因为范围的开头在资源的开头之前，或因为范围的结尾在资源的结尾之后。 ExpectationFailed 417 指示服务器未能符合 Expect 头中给定的预期值。 UpgradeRequired 426 客户端应当切换到TLS/1.0 InternalServerError 500 指示服务器上发生了一般错误。 NotImplemented 501 指示服务器不支持请求的函数。 BadGateway 502 指示中间代理服务器从另一代理或原始服务器接收到错误响应。 ServiceUnavailable 503 指示服务器暂时不可用，通常是由于过多加载或维护。 GatewayTimeout 504 指示中间代理服务器在等待来自另一个代理或原始服务器的响应时已超时。 HttpVersionNotSupported 505 指示服务器不支持请求的 HTTP 版本。","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"},{"name":"前端","slug":"前端","permalink":"https://wxzhongwang.github.io/tags/前端/"}]},{"title":"Git使用中的问题","date":"2018-01-08T04:20:02.000Z","path":"2018/01/08/Git使用中的问题/","text":"git push失败 fatal: Could not read from remote repository 阐述问题git push失败 fatal: Could not read from remote repository. 因为仓库地址不对。更改地址就可以push了。 问题原因12$ git remote -v$ git remote set-url origin XXX 服务器上的 Git - 生成 SSH 公钥生成 SSH 公钥大多数 Git 服务器都会选择使用 SSH 公钥来进行授权。系统中的每个用户都必须提供一个公钥用于授权，没有的话就要生成一个。生成公钥的过程在所有操作系统上都差不多。首先先确认一下是否已经有一个公钥了。SSH 公钥默认储存在账户的主目录下的 ~/.ssh 目录。若想在github中使用的话需要将公钥复制到github&gt;setting&gt;SSH and GPG keys中添加ssh keys。 1234生成钥匙$ ssh-keygen查看公钥$cat ~/.ssh/id_rsa.pub","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"优秀的开源项目","date":"2017-12-08T02:18:02.000Z","path":"2017/12/08/优秀的开源项目及优秀文章地址/","text":"此部分总结平时工作中积累的项目或者见过的优秀开源项目的总结 优秀的开源项目此部分总结平时工作中积累的项目或者见过的优秀开源项目的总结 前端相关NET相关NodeJS相关NodeJS 中文社区开源官网地址: https://cnodejs.org/ 项目开源地址： https://github.com/cnodejs/nodeclub/ JAVA相关数据库相关其他架构相关优秀文章地址此部分总结平时工作中积累的优秀文章或者博客 前端、JAVA、Python https://www.jqhtml.com/ Nginx:详细文档: http://tengine.taobao.org/book/index.html Jekins:教程http://blog.51cto.com/12832314/2140304","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"}]},{"title":"Redis和Memcached比较","date":"2017-09-12T10:33:59.000Z","path":"2017/09/12/Redis和Memcached比较/","text":"Redis和Memcache都是将数据存放在内存中，都是内存数据库。本文介绍两者的区别。 Redis和Memcached比较 Memcached是多线程，而Redis使用单线程. Memcached使用预分配的内存池的方式，Redis使用现场申请内存的方式来存储数据，并且可以配置虚拟内存。 Redis可以实现持久化，主从复制，实现故障恢复。 Memcached只是简单的key与value,但是Redis支持数据类型比较多。 Redis的存储分为内存存储、磁盘存储 .从这一点，也说明了Redis与Memcached是有区别的。Redis 与Memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改 操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 Redis有两种存储方式(默认:snapshot) snapshot 实现方法是定时将内存的快照(snapshot)持久化到硬盘，这种方法缺点是持久化之后如果出现crash则会丢失一段数据。因此在完美主义者的推动下作者增加了aof方式。 aof 即append only mode，在写入内存数据的同时将操作命令保存到日志文件，在一个并发更改上万的系统中，命令日志是一个非常庞大的数据，管理维护成本非常高，恢复重建时间会非常长，这样导致失去aof高可用性本意。另外更重要的是Redis是一个内存数据结构模型，所有的优势都是建立在对内存复杂数据结构高效的原子操作","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"},{"name":"NOSQL","slug":"NOSQL","permalink":"https://wxzhongwang.github.io/tags/NOSQL/"}]},{"title":"Redis简介","date":"2017-09-12T10:33:59.000Z","path":"2017/09/12/Redis简介/","text":"Redis是一个开源的，使用C语言编写，面向“键/值”对类型数据的分布式NoSQL数据库系统，特点是高性能，持久存储，适应高并发的应用场景。Redis纯粹为应用而产生，它是一个高性能的key-value数据库,并且提供了多种语言的API，性能测试结果表示SET操作每秒钟可达110000次，GET操作每秒81000次（当然不同的服务器配置性能不同）。 RedisRedis是一个开源的，使用C语言编写，面向“键/值”对类型数据的分布式NoSQL数据库系统，特点是高性能，持久存储，适应高并发的应用场景。Redis纯粹为应用而产生，它是一个高性能的key-value数据库,并且提供了多种语言的API，性能测试结果表示SET操作每秒钟可达110000次，GET操作每秒81000次（当然不同的服务器配置性能不同）。 Redis目前提供五种数据类型： string(字符串), list（链表）, Hash（哈希）, set（集合）, zset(sorted set) （有序集合） Redis开发维护很活跃，虽然它是一个Key-Value数据库存储系统，但它本身支持MQ功能，所以完全可以当做一个轻量级的队列服务来使用。 Redis可以做消息队列？ 首先，redis设计用来做缓存的，但是由于它自身的某种特性使得它可以用来做消息队列，它有几个阻塞式的API可以使用，正是这些阻塞式的API让其有能力做消息队列；另外，做消息队列的其他特性例如FIFO（先入先出）也很容易实现，只需要一个list对象从头取数据，从尾部塞数据即可；redis能做消息队列还得益于其list对象blpop brpop接口以及Pub/Sub（发布/订阅）的某些接口，它们都是阻塞版的，所以可以用来做消息队列。 对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。实验表明： 入队时，当数据比较小时Redis的性能要高于RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ的出队性能则远低于Redis。","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"},{"name":"消息队列","slug":"消息队列","permalink":"https://wxzhongwang.github.io/tags/消息队列/"}]}]