[{"title":"Java VM 1.0","date":"2019-01-30T08:55:20.159Z","path":"2019/01/30/1.0 Java程序执行流程/","text":"1.Java程序执行流程 Java技术的核心就是Java虚拟机，因为所有的Java程序都在虚拟机上运行。Java程序的运行需要Java虚拟机、Java API和Java Class文件的配合。Java虚拟机实例负责运行一个Java程序。当启动一个Java程序时，一个虚拟机实例就诞生了。当程序结束，这个虚拟机实例也就消亡。 分为编译时环境和运行时环境，很基础很重要。","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"Java VM 2.0","date":"2019-01-30T08:55:16.884Z","path":"2019/01/30/2.0 Java虚拟机/","text":"#Java虚拟机 作用： Java虚拟机的主要任务是装载class文件并且执行其中的字节码。Java虚拟机包含一个类装载器（class loader），它可以从程序和API中装载class文件，Java API中只有程序执行时需要的类才会被装载，字节码由执行引擎来执行。 当Java虚拟机由主机操作系统上的软件实现时，Java程序通过调用本地方法和主机进行交互。Java方法由Java语言编写，编译成字节码，存储在class文件中。本地方法由C/C++/汇编语言编写，编译成和处理器相关的机器代码，存储在动态链接库中，格式是各个平台专有。所以本地方法是联系Java程序和底层主机操作系统的连接方式。（跨平台）","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"Java VM 4.0","date":"2019-01-30T08:55:14.146Z","path":"2019/01/30/4.0 Class文件/","text":"Class文件Class文件是一组以8位字节为基础单位的二进制流，各个数据项目严格按照顺序紧凑地排列在Class文件中，中间没有添加任何分隔符号。Class文件采用一种类似于C语言结构体的伪结构来存储数据，这种伪结构中只包含两种数据类型，无符号数和表。无符号数属于基本的数据类型，以u1,u2,u4,u8分别代表1个字节，2个字节，4个字节和8个字节的无符号数，可以用来描述数字、索引引用、数量值或者按照utf-8编码构成字符串值。表是由多个无符号数或者其他表作为数据项构成的复合数据类型，所有表都习惯性地以“_info”结尾，用来描述有层次关系的复合结构数据。 Class文件的内容包括： 1234567891011121314151617181920ClassFile &#123; u4 magic; //魔数：0xCAFEBABE，用来判断是否是Java class文件 u2 minor_version; //次版本号 u2 major_version; //主版本号 u2 constant_pool_count; //常量池大小 cp_info constant_pool[constant_pool_count-1]; //常量池 u2 access_flags; //类和接口层次的访问标志（通过|运算得到） u2 this_class; //类索引（指向常量池中的类常量） u2 super_class; //父类索引（指向常量池中的类常量） u2 interfaces_count; //接口索引计数器 u2 interfaces[interfaces_count]; //接口索引集合 u2 fields_count; //字段数量计数器 field_info fields[fields_count]; //字段表集合 u2 methods_count; //方法数量计数器 method_info methods[methods_count]; //方法表集合 u2 attributes_count; //属性个数 attribute_info attributes[attributes_count]; //属性表&#125; 访问标志：类还是接口；是否定义为public类型，abstract类型；若为类，是否声明为final。 字段：用来描述接口或类中的变量，但不包括在方法内部的变量。 字面量：文本字符串，声明为final的常量值等。 符号引用：类和接口的全限定名；字段的名称和描述符；方法的名称和描述符。 类索引，父类索引，接口索引：用来确定类的继承关系。","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"Java VM 3.0","date":"2019-01-30T08:55:10.612Z","path":"2019/01/30/3.0 Java虚拟机的内部体系结构/","text":"Java虚拟机的内部体系结构在 Java虚拟机规范中，一个虚拟机实例的行为是分别按照子系统、内存区、数据类型和指令来描述的，这些组成部分一起展示了抽象的虚拟机内部体系结构。 运行时数据区Java虚拟机在执行Java程序的时候会把它管理的内存划分为若干个不同的数据区域，这些区域有各自的用途以及创建和销毁的时机，有的区域随着虚拟机进程的启动而存在（线程共享），有的区域则随着用户线程的启动和结束而建立和销毁（线程私有）。Java虚拟机所管理的内存包括以下几个运行时数据区域。 程序计数器对于一个运行中的Java程序而言，每一个线程都有它的程序计数器，也叫PC寄存器，可以看做当前线程所执行的字节码的行号指示器。在虚拟机的概念模型里，字节码解释器就是通过改变这个计数器的值来选取下一条需要执行的字节码指令，分支，循环，跳转，异常处理，线程恢复等都需要依赖这个计数器。 程序计数器既能持有一个本地指针，也能持有一个returnAddress。当线程执行某个Java方法时，程序计数器的值总是下一条被执行指令的地址。这里的地址可以是一个本地指针，也可以是方法字节码中相对该方法起始指令的偏移量。如果该线程正在执行一个本地方法，那么此时程序计数器的值是“undefined”。 程序计数器属于线程私有的内存，也就是说，每当创建一个线程，都将得到该线程自己的一个程序计数器。Java虚拟机的多线程是通过线程的轮换并且分配处理器的执行时间来实现的，在一个确定的时刻，一个处理器都只会执行一条线程中的指令，为了线程切换后能恢复到正确的执行位置，每条线程都需要有一个独立的程序计数器。 Java虚拟机栈（Java栈）Java虚拟机栈也是线程私有的，它的生命周期和线程相同。Java虚拟机栈描述的是Java方法执行的内存模型，每个方法在执行时会创建一个栈帧，用于存放局部变量表，操作数栈，动态链接，方法出口等信息。一个Java方法从调用到执行结束的过程，相当于一个栈帧在Java虚拟机栈中从入栈到出栈的过程。（1）局部变量表局部变量表用于存放编译时期可知的各种基本数据类型，对象的引用（不等同于对象，可能是指向对象的起始地址的指针，也可能是指向一个代表对象的句柄）以及returnAddress类型（指向了一条字节码地址）。局部变量在方法执行时被创建，在方法执行结束时销毁。 字节码指令通过从0开始的索引使用其中的数据。类型为int, float, reference和returnAddress的值在数组中占据一项，而类型为byte, short和char的值在存入数组前都被转换为int值，也占据一项。但类型为long和double的值在数组中却占据连续的两项。 操作数栈和局部变量区一样，操作数栈也是被组织成一个以字长为单位的数组。它通过标准的栈操作访问–压栈和出栈。由于程序计数器无法被程序指令直接访问，Java虚拟机的指令是从操作数栈中取得操作数，所以它的运行方式是基于栈而不是基于寄存器。虚拟机把操作数栈作为它的工作区，因为大多数指令都要从这里弹出数据，执行运算，然后把结果压回操作数栈。 帧数据区除了局部变量区和操作数栈，Java栈帧还需要帧数据区来支持常量池解析、正常方法返回以及异常派发机制。每当虚拟机要执行某个需要用到常量池数据的指令时，它会通过帧数据区中指向常量池的指针来访问它。除了常量池的解析外，帧数据区还要帮助虚拟机处理Java方法的正常结束或异常中止。如果通过return正常结束，虚拟机必须恢复发起调用的方法的栈帧，包括设置程序计数器指向发起调用方法的下一个指令；如果方法有返回值，虚拟机需要将它压入到发起调用的方法的操作数栈。为了处理Java方法执行期间的异常退出情况，帧数据区还保存一个对此方法异常表的引用。 本地方法栈任何本地方法接口都会使用某种本地方法栈，本地方法栈与Java虚拟机栈发挥的作用类似。当线程调用Java方法时，虚拟机会创建一个新的栈帧并压入Java栈。当它调用的是本地方法时，虚拟机会保持Java栈不变，不再在线程的Java栈中压入新的栈，虚拟机只是简单地动态连接并直接调用指定的本地方法。 堆Java程序在运行时创建的所有类实例或数组（数组在Java虚拟机中是一个真正的对象）都放在同一个堆中，堆是虚拟机管理的内存最大的一块，被所有线程所共享，在虚拟机启动的时候创建。堆内存的唯一目的就是存放对象实例，几乎所有的对象实例都在堆中分配内存。 方法区方法区同Java堆一样，是各个线程共享的内存区域，用于存储已经被虚拟机加载的类信息，常量，静态变量以及及时编译器编译后的代码等信息。当虚拟机装载某个类型时，它使用类装载器定位相应的class文件，然后读入这个class文件并将它传输到虚拟机中，接着虚拟机提取其中的类型信息，并将这些信息存储到方法区。方法区也可以被垃圾回收器收集，因为虚拟机允许通过用户定义的类装载器来动态扩展Java程序。 方法区中存放了以下信息： • 这个类型的全限定名（如全限定名java.lang.Object） • 这个类型的直接超类的全限定名 • 这个类型是类类型还是接口类型 • 这个类型的访问修饰符（public, abstract, final的某个子集） • 任何直接超接口的全限定名的有序列表 • 该类型的常量池，Class文件中除了有类的版本，字段，方法，接口等描述信息外，还有一项是常量池，用于存放编译时生成的各种字面量和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放 • 字段信息（字段名、类型、修饰符） • 方法信息（方法名、返回类型、参数数量和类型、修饰符） • 除了常量以外的所有类（静态）变量 • 指向ClassLoader类的引用（每个类型被装载时，虚拟机必须跟踪它是由启动类装载器还是由用户自定义类装载器装载的） • 指向Class类的引用（对于每一个被装载的类型，虚拟机相应地为它创建一个java.lang.Class类的实例存于堆中。比如你有一个到java.lang.Integer类的对象的引用，那么只需要调用Integer对象引用的getClass()方法，就可以得到表示java.lang.Integer类的Class对象） 类加载子系统虚拟机把Java描述类的信息加载到内存，并对数据进行校验，转换解析和初始化，形成可以被Java虚拟机直接使用的Java类型，这就是Java虚拟机的类加载机制。类从加载到虚拟机开始，到卸载出内存为止，它的整个生命周期包括： 加载，验证，准备，解析，初始化，使用和卸载7个阶段。 加载加载是类加载的一个阶段，在该阶段，Java虚拟机主要完成以下三件事情： • 根据此类的全限定名来确定该类的二进制字节流； • 将这个字节流所代表的静态数据存储结构转换为方法区的运行时数据结构； • 在堆内存中生成一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。 验证验证是连接的第一步，这一阶段的目的是为了确保Class文件的字节流中包含的信息是否符合该虚拟机的要求。 准备准备阶段正式为类变量在方法区中分配内存并且赋初始值，初始值一般为该类变量类型的零值。 解析解析阶段虚拟机将常量池内的符号引用替换为直接引用。符号引用：以一组符号来描述所引用的目标，符号可以是任何形式的字面量，只要使用时能无歧义的定位到目标即可。直接引用：是指能直接指向目标的指针，相对偏移量或是一个能间接定位到目标的句柄。 初始化初始化是类加载过程的最后一个阶段，在前面的类加载过程中，除了在加载阶段用户可以自定义类加载器参与之外，其余动作完全由虚拟机主导和控制，到了初始化阶段，才真正开始执行类中定义的Java代码（字节码）。准备阶段为类变量分配内存并且赋初始值，赋值操作在初始化阶段执行。","tags":[{"name":"Java","slug":"Java","permalink":"https://wxzhongwang.github.io/tags/Java/"}]},{"title":"Docker初探1.0","date":"2019-01-30T08:54:02.318Z","path":"2019/01/30/Docker基本命令/","text":"Docker命令12从公网拉取一个镜像docker pull images_name 12查看已有的docker镜像docker images 12查看帮助docker command --help 12看容器的端口映射情况docker port con_id 12查看正在运行的容器docker ps 12查看所有的容器docker ps -a","tags":[{"name":"Docker","slug":"Docker","permalink":"https://wxzhongwang.github.io/tags/Docker/"}]},{"title":"Git 远程仓库","date":"2019-01-30T08:42:39.563Z","path":"2019/01/30/Git 远程仓库/","text":"Git 远程仓库为了能在任意 Git 项目上协作，你需要知道如何管理自己的远程仓库。远程仓库是指托管在因特网或其他网络中的你的项目的版本库。你可以有好几个远程仓库，通常有些仓库对你只读，有些则可以读写。 与他人协作涉及管理远程仓库以及根据需要推送或拉取数据。管理远程仓库包括了解如何添加远程仓库、移除无效的远程仓库、管理不同的远程分支并定义它们是否被跟踪等等。 查看远程仓库如果想查看你已经配置的远程仓库服务器，可以运行 git remote 命令。 它会列出你指定的每一个远程服务器的简写。 如果你已经克隆了自己的仓库，那么至少应该能看到 origin - 这是 Git 给你克隆的仓库服务器的默认名字：12345678910$ git clone git@172.16.5.77:shengwangzhong/storelcoator.gitCloning into 'storelcoator'...remote: Reusing existing pack: 1857, done.remote: Total 1857 (delta 0), reused 0 (delta 0)Receiving objects: 100% (1857/1857), 374.35 KiB | 268.00 KiB/s, done.Resolving deltas: 100% (772/772), done.Checking connectivity... done.$ cd storelcoator$ git remoteorigin 你也可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL。123$ git remote -vorigin git@172.16.5.77:shengwangzhong/vue-myblog.git (fetch)origin git@172.16.5.77:shengwangzhong/vue-myblog.git (push) 如果你的远程仓库不止一个，该命令会将它们全部列出。 例如，与几个协作者合作的，拥有多个远程仓库的仓库看起来像下面这样：123456789101112$ cd grit$ git remote -vbakkdoor https://github.com/bakkdoor/grit (fetch)bakkdoor https://github.com/bakkdoor/grit (push)cho45 https://github.com/cho45/grit (fetch)cho45 https://github.com/cho45/grit (push)defunkt https://github.com/defunkt/grit (fetch)defunkt https://github.com/defunkt/grit (push)koke git://github.com/koke/grit.git (fetch)koke git://github.com/koke/grit.git (push)origin git@github.com:mojombo/grit.git (fetch)origin git@github.com:mojombo/grit.git (push) 添加远程仓库运行 git remote add 添加一个新的远程 Git 仓库，同时指定一个你可以轻松引用的简写：12345678$ git remoteorigin$ git remote add pb https://github.com/paulboone/ticgit$ git remote -vorigin https://github.com/schacon/ticgit (fetch)origin https://github.com/schacon/ticgit (push)pb https://github.com/paulboone/ticgit (fetch)pb https://github.com/paulboone/ticgit (push) 现在你可以在命令行中使用字符串 pb 来代替整个 URL。 例如，如果你想拉取 Paul 的仓库中有但你没有的信息，可以运行 git fetch pb：12345678$ git fetch pbremote: Counting objects: 43, done.remote: Compressing objects: 100% (36/36), done.remote: Total 43 (delta 10), reused 31 (delta 5)Unpacking objects: 100% (43/43), done.From https://github.com/paulboone/ticgit * [new branch] master -&gt; pb/master * [new branch] ticgit -&gt; pb/ticgit 现在 Paul 的 master 分支可以在本地通过 pb/master 访问到 - 你可以将它合并到自己的某个分支中，或者如果你想要查看它的话，可以检出一个指向该点的本地分支。 从远程仓库中抓取与拉取就如刚才所见，从远程仓库中获得数据，可以执行：1$ git fetch [remote-name] 这个命令会访问远程仓库，从中拉取所有你还没有的数据。 执行完成后，你将会拥有那个远程仓库中所有分支的引用，可以随时合并或查看。 如果你使用 clone 命令克隆了一个仓库，命令会自动将其添加为远程仓库并默认以 “origin” 为简写。 所以，git fetch origin 会抓取克隆（或上一次抓取）后新推送的所有工作。 必须注意 git fetch 命令会将数据拉取到你的本地仓库 - 它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。 如果你有一个分支设置为跟踪一个远程分支，可以使用 git pull 命令来自动的抓取然后合并远程分支到当前分支。这对你来说可能是一个更简单或更舒服的工作流程；==默认情况下，git clone 命令会自动设置本地 master 分支跟踪克隆的远程仓库的 master 分支（或不管是什么名字的默认分支）。== 运行 git pull 通常会从最初克隆的服务器上抓取数据并自动尝试合并到当前所在的分支。 推送到远程仓库当你想分享你的项目时，必须将其推送到上游。 这个命令很简单：git push [remote-name] [branch-name]。 当你想要将 master 分支推送到 origin 服务器时（再次说明，克隆时通常会自动帮你设置好那两个名字），那么运行这个命令就可以将你所做的备份到服务器：1$ git push origin master 只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。 当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。 你必须先将他们的工作拉取下来并将其合并进你的工作后才能推送。 查看远程仓库(详细)1234567891011$ git remote show origin* remote origin Fetch URL: git@172.16.5.77:shengwangzhong/vue-myblog.git Push URL: git@172.16.5.77:shengwangzhong/vue-myblog.git HEAD branch: master Remote branch: master tracked Local branch configured for 'git pull': master merges with remote master Local ref configured for 'git push': master pushes to master (up to date) 这个命令列出了当你在特定的分支上执行 git push 会自动地推送到哪一个远程分支。 它也同样地列出了哪些远程分支不在你的本地，哪些远程分支已经从服务器上移除了，还有当你执行 git pull 时哪些分支会自动合并。 远程仓库的移除与重命名如果想要重命名引用的名字可以运行 git remote rename 去修改一个远程仓库的简写名。 例如，想要将 pb 重命名为 paul，可以用 git remote rename 这样做：1234$ git remote rename pb paul$ git remoteoriginpaul","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git 撤销操作","date":"2019-01-30T08:42:35.376Z","path":"2019/01/30/Git 撤销操作/","text":"Git 撤销操作在任何一个阶段，你都有可能想要撤消某些操作。 注意，有些撤消操作是不可逆的。 这是在使用 Git 的过程中，会因为操作失误而导致之前的工作丢失的少有的几个地方之一。 撤销操作有时候我们提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了。 此时，可以运行带有 –amend 选项的提交命令尝试重新提交：1$ git commit --amend 这个命令会将暂存区中的文件提交。如果自上次提交以来你还未做任何修改（例如，在上次提交后马上执行了此命令），那么快照会保持不变，而你所修改的只是提交信息。 例如，你提交后发现忘记了暂存某些需要的修改，可以像下面这样操作：123$ git commit -m 'initial commit'$ git add forgotten_file$ git commit --amend 最终你只会有一个提交 - 第二次提交将代替第一次提交的结果。 取消暂存的文件接下来的两个小节演示如何操作暂存区域与工作目录中已修改的文件。 这些命令在修改文件状态的同时，也会提示如何撤消操作。 例如，你已经修改了两个文件并且想要将它们作为两次独立的修改提交，但是却意外地输入了 git add * 暂存了它们两个。 如何只取消暂存两个中的一个呢？ git status 命令提示了你：12345678$ git add *$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) renamed: README.md -&gt; README modified: CONTRIBUTING.md 在 “Changes to be committed” 文字正下方，提示使用 git reset HEAD … 来取消暂存。 所以，我们可以这样来取消暂存 CONTRIBUTING.md 文件：123456789101112131415$ git reset HEAD CONTRIBUTING.mdUnstaged changes after reset:M CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) renamed: README.md -&gt; READMEChanges not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: CONTRIBUTING.md 这个命令有点儿奇怪，但是起作用了。 CONTRIBUTING.md 文件已经是修改未暂存的状态了。 Note：虽然在调用时加上 –hard 选项可以令 git reset 成为一个危险的命令（译注：可能导致工作目录中所有当前进度丢失！）不加选项地调用 git reset 并不危险 — 它只会修改暂存区域。 撤消对文件的修改如果你并不想保留对 CONTRIBUTING.md 文件的修改怎么办？ 你该如何方便地撤消修改 - 将它还原成上次提交时的样子（或者刚克隆完的样子，或者刚把它放入工作目录时的样子）？ 幸运的是，git status 也告诉了你应该如何做。 在最后一个例子中，未暂存区域是这样：12345Changes not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: CONTRIBUTING.md 它非常清楚地告诉了你如何撤消之前所做的修改。 让我们来按照提示执行：1234567$ git checkout -- CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) renamed: README.md -&gt; README 可以看到那些修改已经被撤消了。 Important: 你需要知道 git checkout – [file] 是一个危险的命令，这很重要。 你对那个文件做的任何修改都会消失 - 你只是拷贝了另一个文件来覆盖它。 除非你确实清楚不想要那个文件了，否则不要使用这个命令。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"世界以痛吻我 我却报之以歌","date":"2019-01-29T09:10:10.000Z","path":"2019/01/29/世界以痛吻我，我却报之以歌/","text":"今年发生不好的事，一切都显得不那么美好，就很多事情都不按大家的预想在发展。就一起都感觉不顺利，2018已经不够友好了，还记得刚刚过去的23岁生日，记得自己的愿望是希望，真挚希望所有的亲朋好友能够健康，健康就够了，钱多钱少不重要，就是健康就够了。。。 但是偏偏还是发生了。 世界以痛吻我 我却报之以歌亲人已仙游，未呈儿孙福，幽魂于千里，如何度思量。 2018年06月14日早9：00点，在杭州，还在床上的我猛的收到了家人群的消息，早已患糖尿病多年的舅舅病逝。尽管心里有多不愿意接受，但是心里也算是做好了准备。在五一回家时间便已经被告知舅舅身体日渐消瘦，骨瘦如柴。我还趁着工作之余，抽空去看望了。当看到那一瞬间，我感觉我整个人就不好了，眼泪一下就出来了，整个人像楞住了一般，久久说不出话，手也不知道往哪里放。当舅舅看到我，一把把我拽住，死死捏着，这一幕到现在我也忘不了… 又是在杭州，2019年01月29日早8: 00点，同样的戏码，姨夫病逝，癌症晚期，距离收到通知，也就短短三个多月时间，大概一百多天，原本一个看上去健健康康的人，就突然离开，就像上帝给你的人生突然上了锁，然后把钥匙给扔掉了的感觉… 醒来又是一天，开始干活，累…","tags":[{"name":"个人","slug":"个人","permalink":"https://wxzhongwang.github.io/tags/个人/"}]},{"title":"Git查看历史提交","date":"2019-01-25T03:58:59.671Z","path":"2019/01/25/Git 查看提交历史/","text":"在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。默认不用任何参数的话，git log 会按提交时间列出所有的更新，最近的更新排在最上面。 正如你所看到的，这个命令会列出每个提交的 SHA-1 校验和、作者的名字和电子邮件地址、提交时间以及提交说明。 Git 查看提交历史在提交了若干更新，又或者克隆了某个项目之后，你也许想回顾下提交历史。 完成这个任务最简单而又有效的工具是 git log 命令。默认不用任何参数的话，git log 会按提交时间列出所有的更新，最近的更新排在最上面。 正如你所看到的，这个命令会列出每个提交的 SHA-1 校验和、作者的名字和电子邮件地址、提交时间以及提交说明。 git log 有许多选项可以帮助你搜寻你所要找的提交， 接下来我们介绍些最常用的。 定制输出格式-p一个常用的选项是 -p，用来显示每次提交的内容差异。 你也可以加上 -2 来仅显示最近两次提交： 该选项除了显示基本信息之外，还附带了每次commit的变化。当进行代码审查，或者快速浏览某个搭档提交的commit所带来的变化的时候，这个参数就非常有用了。1234567891011121314151617181920212223242526272829303132333435363738394041$ git log -p -2commit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the version numberdiff --git a/Rakefile b/Rakefileindex a874b73..8f94139 100644--- a/Rakefile+++ b/Rakefile@@ -5,7 +5,7 @@ require 'rake/gempackagetask' spec = Gem::Specification.new do |s| s.platform = Gem::Platform::RUBY s.name = \"simplegit\"- s.version = \"0.1.0\"+ s.version = \"0.1.1\" s.author = \"Scott Chacon\" s.email = \"schacon@gee-mail.com\" s.summary = \"A simple gem for using Git in Ruby code.\"commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary testdiff --git a/lib/simplegit.rb b/lib/simplegit.rbindex a0a60ae..47c6340 100644--- a/lib/simplegit.rb+++ b/lib/simplegit.rb@@ -18,8 +18,3 @@ class SimpleGit end end--if $0 == __FILE__- git = SimpleGit.new- puts git.show-end\\ No newline at end of file –stat你也可以为 git log 附带一系列的总结性选项。比如说，如果你想看到每次提交的简略的统计信息，你可以使用 –stat 选项：1234567891011121314151617181920212223242526272829$ git log --statcommit ca82a6dff817ec66f44342007202690a93763949Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Mon Mar 17 21:52:11 2008 -0700 changed the version number Rakefile | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-)commit 085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 16:40:33 2008 -0700 removed unnecessary test lib/simplegit.rb | 5 ----- 1 file changed, 5 deletions(-)commit a11bef06a3f659402fe7563abf99ad00de2209e6Author: Scott Chacon &lt;schacon@gee-mail.com&gt;Date: Sat Mar 15 10:31:28 2008 -0700 first commit README | 6 ++++++ Rakefile | 23 +++++++++++++++++++++++ lib/simplegit.rb | 25 +++++++++++++++++++++++++ 3 files changed, 54 insertions(+) 正如你所看到的，–stat 选项在每次提交的下面列出所有被修改过的文件、有多少文件被修改了以及被修改过的文件的哪些行被移除或是添加了。 在每次提交的最后还有一个总结。 –pretty 这个选项可以指定使用不同于默认格式的方式展示提交历史。 这个选项有一些内建的子选项供你使用。 比如用 oneline 将每个提交放在一行显示，查看的提交数很大时非常有用。 另外还有 short，full 和 fuller 可以用，展示的信息或多或少有些不同。1234$ git log --pretty=onelineca82a6dff817ec66f44342007202690a93763949 changed the version number085bb3bcb608e1e8451d4b2432f8ecbe6306e7e7 removed unnecessary testa11bef06a3f659402fe7563abf99ad00de2209e6 first commit 但最有意思的是 format，可以定制要显示的记录格式。 这样的输出对后期提取分析格外有用。因为你知道输出的格式不会随着 Git 的更新而发生改变：1234$ git log --pretty=format:\"%h - %an, %ar : %s\"ca82a6d - Scott Chacon, 6 years ago : changed the version number085bb3b - Scott Chacon, 6 years ago : removed unnecessary testa11bef0 - Scott Chacon, 6 years ago : first commit git log –pretty=format 常用的选项 列出了常用的格式占位符写法及其代表的意义。 选项 说明 %H 提交对象（commit）的完整哈希字串 %h 提交对象的简短哈希字串 %T 树对象（tree）的完整哈希字串 %t 树对象的简短哈希字串 %P 父对象（parent）的完整哈希字串 %p 父对象的简短哈希字串 %an 作者（author）的名字 %ae 作者的电子邮件地址 %ad 作者修订日期（可以用 –date= 选项定制格式） %ar 作者修订日期，按多久以前的方式显示 %cn 提交者（committer）的名字 %ce 提交者的电子邮件地址 %cd 提交日期 %cr 提交日期，按多久以前的方式显示 %s 提交说明 你一定奇怪 作者 和 提交者之间究竟有何差别，其实作者指的是实际作出修改的人，提交者指的是最后将此工作成果提交到仓库的人。所以，当你为某个项目发布补丁，然后某个核心成员将你的补丁并入项目时，你就是作者，而那个核心成员就是提交者。 –graph当 oneline 或 format 与另一个 log 选项 –graph 结合使用时尤其有用。 这个选项添加了一些ASCII字符串来形象地展示你的分支、合并历史：1234567891011$ git log --pretty=format:\"%h %s\" --graph* 2d3acf9 ignore errors from SIGCHLD on trap* 5e3ee11 Merge branch 'master' of git://github.com/dustin/grit|\\| * 420eac9 Added a method for getting the current branch.* | 30e367c timeout code and tests* | 5a09431 add timeout protection to grit* | e1193f8 support for heads with slashes in them|/* d6016bc require time for xmlschema* 11d191e Merge branch 'defunkt' into local 常用汇总 选项 说明 -p 按补丁格式显示每个更新之间的差异。 –stat 显示每次更新的文件修改统计信息。 –shortstat 只显示 –stat 中最后的行数修改添加移除统计。 –name-only 仅在提交信息后显示已修改的文件清单。 –name-status 显示新增、修改、删除的文件清单。 –abbrev-commit 仅显示 SHA-1 的前几个字符，而非所有的 40 个字符。 –relative-date 使用较短的相对时间显示（比如，“2 weeks ago”）。 –graph 显示 ASCII 图形表示的分支合并历史。 –pretty 使用其他格式显示历史提交信息。可用的选项包括 oneline，short，full，fuller 和 format（后跟指定格式）。 限制输出长度it log 还有许多非常实用的限制输出长度的选项，也就是只输出部分提交信息。 之前你已经看到过 -2 了，它只显示最近的两条提交， 实际上，这是 - 选项的写法，其中的 n 可以是任何整数，表示仅显示最近的若干条提交。 不过实践中我们是不太用这个选项的，Git 在输出所有提交时会自动调用分页程序，所以你一次只会看到一页的内容。 常用汇总 选项 说明 -(n) 仅显示最近的 n 条提交。 –since, –after 仅显示指定时间之后的提交。 –until, –before 仅显示指定时间之前的提交。 –author 仅显示指定作者相关的提交。 –committer 仅显示指定提交者相关的提交。 –grep 仅显示含指定关键字的提交。 -S 仅显示添加或移除了某个关键字的提交。 另外还有按照时间作限制的选项，比如 –since 和 –until 也很有用。 例如，下面的命令列出所有最近两周内的提交：1$ git log --since=2.weeks 这个命令可以在多种格式下工作，比如说具体的某一天 “2008-01-15”，或者是相对地多久以前 “2 years 1 day 3 minutes ago”。 来看一个实际的例子，如果要查看 Git 仓库中，2008 年 10 月期间，Junio Hamano 提交的但未合并的测试文件，可以用下面的查询命令：12345678$ git log --pretty=\"%h - %s\" --author=gitster --since=\"2008-10-01\" \\ --before=\"2008-11-01\" --no-merges -- t/5610e3b - Fix testcase failure when extended attributes are in useacd3b9e - Enhance hold_lock_file_for_&#123;update,append&#125;() APIf563754 - demonstrate breakage of detached checkout with symbolic link HEADd1a43f2 - reset --hard/read-tree --reset -u: remove unmerged new paths51a94af - Fix \"checkout --track -b newbranch\" on detached HEADb0ad11e - pull: allow \"git pull origin $something:$current_branch\" into an unborn branch","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git移动文件","date":"2019-01-25T03:58:56.881Z","path":"2019/01/25/Git 移动文件/","text":"移动文件Git 并不显式跟踪文件移动操作。如果在Git中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。 不过 Git 非常聪明，它会推断出究竟发生了什么。 既然如此，当你看到 Git 的 mv 命令时一定会困惑不已。 要在 Git 中对文件改名，可以这么做：1$ git mv file_from file_to 它会恰如预期般正常工作。实际上，即便此时查看状态信息，也会明白无误地看到关于重命名操作的说明：1234567$ git mv README.md README$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) renamed: README.md -&gt; README 其实，运行 git mv 就相当于运行了下面三条命令：123$ mv README.md README$ git rm README.md$ git add README 如此分开操作，Git 也会意识到这是一次改名，所以不管何种方式结果都一样。 两者唯一的区别是，mv 是一条命令而另一种方式需要三条命令，直接用 git mv 轻便得多。 不过有时候用其他工具批处理改名的话，要记得在提交前删除老的文件名，再添加新的文件名。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git移除文件","date":"2019-01-25T03:58:53.780Z","path":"2019/01/25/Git 移除文件/","text":"git rm要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。 可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。 如果只是简单地从工作目录中手工删除文件，运行 git status 时就会在 “Changes not staged for commit” 部分（也就是 未暂存清单）看到：1234567891011$ rm PROJECTS.md$ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Changes not staged for commit: (use \"git add/rm &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) deleted: PROJECTS.mdno changes added to commit (use \"git add\" and/or \"git commit -a\") 然后再运行 git rm 记录此次移除文件的操作：12345678$ git rm PROJECTS.mdrm 'PROJECTS.md'$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) deleted: PROJECTS.md 下一次提交时，该文件就不再纳入版本管理了。如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -f（译注：即force的首字母）。这是一种安全特性，用于防止误删还没有添加到快照的数据，这样的数据不能被 Git 恢复。 另外一种情况是，我们想把文件从Git仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。 换句话说，你想让文件保留在磁盘，但是并不想让 Git 继续跟踪。 当你忘记添加 .gitignore 文件，不小心把一个很大的日志文件或一堆.a这样的编译生成文件添加到暂存区时，这一做法尤其有用。 为达到这一目的，使用 –cached 选项：1$ git rm --cached README git rm 命令后面可以列出文件或者目录的名字，也可以使用 glob 模式。 比方说：1$ git rm log/\\*.log 注意到星号 * 之前的反斜杠 \\， 因为 Git 有它自己的文件模式扩展匹配方式，所以我们不用 shell 来帮忙展开。 此命令删除 log/ 目录下扩展名为 .log 的所有文件。 类似的比如：1$ git rm \\*~ 该命令为删除以 ~ 结尾的所有文件。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git提交","date":"2019-01-25T03:58:48.533Z","path":"2019/01/25/Git Commit/","text":"Git Commit现在的暂存区域已经准备妥当可以提交了。 在此之前，请一定要确认还有什么修改过的或新建的文件还没有 git add 过，否则提交的时候不会记录这些还没暂存起来的变化。 这些修改过的文件只保留在本地磁盘。 所以，每次准备提交前，先用 git status 看下，是不是都已暂存起来了， 然后再运行提交命令 git commit：1$ git commit 你也可以在 commit 命令后添加 -m 选项，将提交信息与命令放在同一行。 请在每次提交时添加comment1234$ git commit -m \"Story 182: Fix benchmarks for speed\"[master 463dc4f] Story 182: Fix benchmarks for speed 2 files changed, 2 insertions(+) create mode 100644 README 好，现在你已经创建了第一个提交！ 可以看到，提交后它会告诉你，当前是在哪个分支（master）提交的，本次提交的完整 SHA-1 校验和是什么（463dc4f），以及在本次提交中，有多少文件修订过，多少行添加和删改过。 请记住，提交时记录的是放在暂存区域的快照。 任何还未暂存的仍然保持已修改状态，可以在下次提交时纳入版本管理。 每一次运行提交操作，都是对你项目作一次快照，以后可以回到这个状态，或者进行比较。 跳过使用暂存区域尽管使用暂存区域的方式可以精心准备要提交的细节，但有时候这么做略显繁琐。 Git 提供了一个跳过使用暂存区域的方式， 只要在提交的时候，给 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤：123456789101112$ git statusOn branch masterChanges not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: CONTRIBUTING.mdno changes added to commit (use \"git add\" and/or \"git commit -a\")$ git commit -a -m 'added new benchmarks'[master 83e38c7] added new benchmarks 1 file changed, 5 insertions(+), 0 deletions(-) 此时，提交之前不再需要 git add 文件“CONTRIBUTING.md”了。个人觉得不推荐。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git Diff","date":"2019-01-24T08:08:56.227Z","path":"2019/01/24/Git Diff/","text":"#Git Diff 如果 git status 命令的输出对于你来说过于模糊，你想知道具体修改了什么地方，可以用 git diff 命令。 尽管 git status 已经通过在相应栏下列出文件名的方式回答了这个问题，git diff 将通过文件补丁的格式显示具体哪些行发生了改变。 作用此命令比较的是工作目录中当前文件和暂存区域快照之间的差异，也就是修改之后还没有暂存起来的变。化内容。 若要查看已暂存的将要添加到下次提交里的内容，可以用 git diff –cached 命令。（Git 1.6.1 及更高版本还允许使用 git diff –staged，效果是相同的，但更好记些。） 请注意，git diff 本身只显示尚未暂存的改动，而不是自上次提交以来所做的所有改动。 所以有时候你一下子暂存了所有更新过的文件后，运行 git diff 后却什么也没有，就是这个原因。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git .gitignore","date":"2019-01-24T08:08:53.509Z","path":"2019/01/24/Git gitignore/","text":".gitignore 忽略文件一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。 通常都是些自动生成的文件，比如日志文件，或者编译过程中创建的临时文件等。 在这种情况下，我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件模式。 来看一个实际的例子： 123$ cat .gitignore*.[oa]*~ 第一行告诉 Git 忽略所有以 .o 或 .a 结尾的文件。一般这类对象文件和存档文件都是编译过程中出现的。 第二行告诉 Git 忽略所有以波浪符（~）结尾的文件，许多文本编辑软件（比如 Emacs）都用这样的文件名保存副本。 此外，你可能还需要忽略 log，tmp 或者 pid 目录，以及自动生成的文档等等。 要养成一开始就设置好 .gitignore 文件的习惯，以免将来误提交这类无用的文件。 文件 .gitignore 的格式规范如下： 所有空行或者以 ＃ 开头的行都会被 Git 忽略。 可以使用标准的 glob 模式匹配。 匹配模式可以以（/）开头防止递归。 匹配模式可以以（/）结尾指定目录。 要忽略指定模式以外的文件或目录，可以在模式前加上惊叹号（!）取反。 所谓的 glob 模式是指 shell 所使用的简化了的正则表达式。 星号（）匹配零个或多个任意字符；[abc] 匹配任何一个列在方括号中的字符（这个例子要么匹配一个 a，要么匹配一个 b，要么匹配一个 c）；问号（?）只匹配一个任意字符；如果在方括号中使用短划线分隔两个字符，表示所有在这两个字符范围内的都可以匹配（比如 [0-9] 表示匹配所有 0 到 9 的数字）。 使用两个星号（) 表示匹配任意中间目录，比如a/**/z 可以匹配 a/z, a/b/z 或 a/b/c/z等。 我们再看一个 .gitignore 文件的例子：1234567891011121314151617# no .a files*.a# but do track lib.a, even though you're ignoring .a files above!lib.a# only ignore the TODO file in the current directory, not subdir/TODO/TODO# ignore all files in the build/ directorybuild/# ignore doc/notes.txt, but not doc/server/arch.txtdoc/*.txt# ignore all .pdf files in the doc/ directorydoc/**/*.pdf","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git记录更新到仓库","date":"2019-01-24T08:08:50.299Z","path":"2019/01/24/Git 基础 - 记录每次更新到仓库/","text":"记录每次更新到仓库现在我们手上有了一个真实项目的 Git 仓库，并从这个仓库中取出了所有文件的工作拷贝。 接下来，对这些文件做些修改，在完成了一个阶段的目标之后，提交本次更新到仓库。 请记住，你工作目录下的每一个文件都不外乎这两种状态：已跟踪或未跟踪。 已跟踪的文件是指那些被纳入了版本控制的文件，在上一次快照中有它们的记录，在工作一段时间后，它们的状态可能处于未修改，已修改或已放入暂存区。 工作目录中除已跟踪文件以外的所有其它文件都属于未跟踪文件，它们既不存在于上次快照的记录中，也没有放入暂存区。 初次克隆某个仓库的时候，工作目录中的所有文件都属于已跟踪文件，并处于未修改状态。 编辑过某些文件之后，由于自上次提交后你对它们做了修改，Git 将它们标记为已修改文件。 我们逐步将这些修改过的文件放入暂存区，然后提交所有暂存了的修改，如此反复。所以使用 Git 时文件的生命周期如下： 1234检查当前文件状态:$ git statusOn branch masternothing to commit, working directory clean 这说明你现在的工作目录相当干净。换句话说，所有已跟踪文件在上次提交后都未被更改过。 此外，上面的信息还表明，当前目录下没有出现任何处于未跟踪状态的新文件，否则 Git 会在这里列出来。 最后，该命令还显示了当前所在分支，并告诉你这个分支同远程服务器上对应的分支没有偏离。 现在，分支名是 “master”,这是默认的分支名。 现在，让我们在项目下创建一个新的 README 文件。 如果之前并不存在这个文件，使用 git status 命令，你将看到一个新的未跟踪文件：123456789$ echo 'My Project' &gt; README$ git statusOn branch masterUntracked files: (use \"git add &lt;file&gt;...\" to include in what will be committed) READMEnothing added to commit but untracked files present (use \"git add\" to track) 在状态报告中可以看到新建的 README 文件出现在 Untracked files 下面。 未跟踪的文件意味着 Git 在之前的快照（提交）中没有这些文件；Git 不会自动将之纳入跟踪范围，除非你明明白白地告诉它“我需要跟踪该文件”， 这样的处理让你不必担心将生成的二进制文件或其它不想被跟踪的文件包含进来。 不过现在的例子中，我们确实想要跟踪管理 README 这个文件。12使用命令 git add 开始跟踪一个文件。 所以，要跟踪 README 文件，运行：$ git add README 这时候我们在运行git status查看状态：123456$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: README 文件已经处于被追踪的状态了，并处于暂存状态。 现在我们来修改一个已被跟踪的文件。 如果你修改了一个名为 CONTRIBUTING.md 的已被跟踪的文件，然后运行 git status 命令，会看到下面内容：123456789101112$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: READMEChanges not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: CONTRIBUTING.md 文件 CONTRIBUTING.md 出现在 Changes not staged for commit 这行下面，说明已跟踪文件的内容发生了变化，但还没有放到暂存区。 要暂存这次更新，需要运行 git add命令。 git add: 这是个多功能命令：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等。 将这个命令理解为“添加内容到下一次提交中”而不是“将一个文件添加到项目中”要更加合适。 现在让我们运行 git add 将”CONTRIBUTING.md”放到暂存区，然后再看看 git status 的输出：12345678$ git add CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: README modified: CONTRIBUTING.md 现在两个文件都已暂存，下次提交时就会一并记录到仓库。 假设此时，你想要在 CONTRIBUTING.md 里再加条注释， 重新编辑存盘后，准备好提交。 不过且慢，再运行 git status 看看：1234567891011121314$ vim CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: README modified: CONTRIBUTING.mdChanges not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: CONTRIBUTING.md 怎么回事？ 现在 CONTRIBUTING.md 文件同时出现在暂存区和非暂存区。 这怎么可能呢？ 好吧，实际上 Git 只不过暂存了你运行 git add 命令时的版本， 如果你现在提交，CONTRIBUTING.md 的版本是你最后一次运行 git add 命令时的那个版本，而不是你运行 git commit 时，在工作目录中的当前版本。 所以，运行了 git add 之后又作了修订的文件，需要重新运行 git add 把最新版本重新暂存起来：12345678$ git add CONTRIBUTING.md$ git statusOn branch masterChanges to be committed: (use \"git reset HEAD &lt;file&gt;...\" to unstage) new file: README modified: CONTRIBUTING.md git statusgit status 命令的输出十分详细，但其用语有些繁琐。 如果你使用 git status -s 命令或 git status –short 命令，你将得到一种更为紧凑的格式输出。 运行 git status -s ，状态报告输出如下：123456$ git status -s M READMEMM RakefileA lib/git.rbM lib/simplegit.rb?? LICENSE.txt 新添加的未跟踪文件前面有 ?? 标记，新添加到暂存区中的文件前面有 A 标记，修改过的文件前面有 M 标记。 你可能注意到了 M 有两个可以出现的位置，出现在右边的 M 表示该文件被修改了但是还没放入暂存区，出现在靠左边的 M 表示该文件被修改了并放入了暂存区。 例如，上面的状态报告显示： README 文件在工作区被修改了但是还没有将修改后的文件放入暂存区,lib/simplegit.rb 文件被修改了并将修改后的文件放入了暂存区。 而 Rakefile 在工作区被修改并提交到暂存区后又在工作区中被修改了，所以在暂存区和工作区都有该文件被修改了的记录。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git Clone","date":"2019-01-24T08:08:46.776Z","path":"2019/01/24/Git 获取Git仓库/","text":"获取 Git 仓库有两种取得 Git 项目仓库的方法。 第一种是在现有项目或目录下导入所有文件到 Git 中 第二种是从一个服务器克隆一个现有的 Git 仓库 在现有目录初始化 1$ git init 该命令将创建一个名为 .git 的子目录，这个子目录含有你初始化的 Git 仓库中所有的必须文件，这些文件是 Git 仓库的骨干。 但是，在这个时候，我们仅仅是做了一个初始化的操作，你的项目里的文件还没有被跟踪。 克隆现有的仓库 如果你想获得一份已经存在了的Git仓库的拷贝，比如说，你想为某个开源项目贡献自己的一份力，这时就要用到 git clone 命令。 克隆仓库的命令格式是 git clone [url] 。 1$ git clone git@172.16.5.77:shengwangzhong/hexo-blog.git 这会在当前目录下创建一个名为 “hexo-blog” 的目录，并在这个目录下初始化一个 .git 文件夹，从远程仓库拉取下所有数据放入.git文件夹，然后从中读取最新版本的文件的拷贝。 如果你进入到这个新建的hexo-blog文件夹，你会发现所有的项目文件已经在里面了，准备就绪等待后续的开发和使用。 如果你想在克隆远程仓库的时候，自定义本地仓库的名字，你可以使用如下命令： 1$ git clone git@172.16.5.77:shengwangzhong/hexo-blog.git your-folder-name 这将执行与上一个命令相同的操作，不过在本地创建的仓库名字变为 your-folder-name。 Git 支持多种数据传输协议。 ssh\\https\\git","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git起步","date":"2019-01-24T08:08:43.060Z","path":"2019/01/24/Git起步/","text":"Git起步 安装好第一步一定是配置用户名和邮箱. 配置用户名邮箱12$ git config --global user.name \"shengwangzhong\"$ git config --global user.email shengwangzhong@hengtiansoft.com 再次强调，如果使用了 –global 选项，那么该命令只需要运行一次，因为之后无论你在该系统上做任何事情， Git 都会使用那些信息。 当你想针对特定项目使用不同的用户名称与邮件地址时，可以在那个项目目录下运行没有 –global 选项的命令来配置。 检查配置如果想要检查你的配置，可以使用 git config –list 命令来列出所有 Git 当时能找到的配置。 123456789101112131415161718192021$ git config --listcore.symlinks=falsecore.autocrlf=truecore.fscache=truecolor.diff=autocolor.status=autocolor.branch=autocolor.interactive=truehelp.format=htmlrebase.autosquash=truehttp.sslcainfo=D:/Git/mingw64/ssl/certs/ca-bundle.crthttp.sslbackend=openssldiff.astextplain.textconv=astextplainfilter.lfs.clean=git-lfs clean -- %ffilter.lfs.smudge=git-lfs smudge -- %ffilter.lfs.required=truefilter.lfs.process=git-lfs filter-processcredential.helper=manageruser.name=shengwangzhonguser.email=shengwangzhong@hengtiansoft.comcredential.helper=store","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git帮助","date":"2019-01-24T08:08:38.524Z","path":"2019/01/24/Git帮助/","text":"Git帮助123若你使用 Git 时需要获取帮助，有三种方法可以找到 Git 命令的使用手册：$ git help &lt;verb&gt;$ git &lt;verb&gt; --help 例如，要想获得 config 命令的手册，执行1$ git help config","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"Git安装","date":"2019-01-24T08:08:31.434Z","path":"2019/01/24/Git安装/","text":"Linux 安装Git:12345678检查是否安装Gitgit --version 在 Ubuntu 这类 Debian 体系的系统上，可以用 apt-get 安装：sudo apt-get install gitCentOs:yum install -y git window安装比较简单不多说。","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"互联网金融行业数仓分层","date":"2019-01-13T03:34:12.678Z","path":"2019/01/13/互联网金融行业数仓分层/","text":"互联网金融行业数仓分层专业术语 ODL层 （Operational Data Layer）：操作数据层 外部数据什么样，该层数据就是什么样（关系型数据库、JSON格式等)部分关系型数据可以直接转IDL层 BDL层 （Base Data Layer）：基础数据层 ODL层经过简单格式化解析后存储到BDL层，常见于JSON日志格式的解析。 IDL层 （Interface Data Layer）：接口层，也称主题表，宽表 由BDL层经过去重、去噪、字典翻译、空值转化，日期格式化、关联JOIN、维度分析等清洗后的数据。如：用户、产品、绑卡、订单、用户行为等明细数据。 ADL层（Application Data Layer）：应用层 ，也称数据集市 通常与需求对接，由IDL层基于某些维度的深度加工统计汇总等操作转化而来，涉及到多个主题以及tmp数据之间的关联JOIN后的结果。 DIC层（Dictionary Data Layer）：字典层 存储一些诸如省、市、县区域表、渠道列表、商品类目等等表数据，可以从数据源直接sqoop生成dic_xxx表，也可以通过odl层转化层dic_表。 TMP层（Temporary Data Layer）：临时层 存储一些中间计算结果 简要说明: 层次间的转换没必要循规蹈矩，按部就班，适当做到灵活，避免重复清洗浪费资源 ODL层干净的关系型数据可以直接转换为IDL层数据，减少计算量 ODL层侧重与外部对接，BDL层/TMP层/IDL层侧重清洗，IDL层和ADL层侧重对外提供应用服务 层数太少不够灵活，太多则在数据推翻重洗耗时，时间成本（一个坑）数据源提供的数据越详细越好，避免后期大量重复的清洗工作。 “星型模型”和“雪花模型”简单解释： （1）星型模型：事实表+维度表（区域、类目、性别…)等多表通过预先JOIN冗余到一张宽表里去，常见IDL层。 （2）雪花模型：在计算的时候，才将事实表跟维度表做join。 现在一般都是采用（1）的模式，为什么呢？ 预先计算，挺高性能，避免后续重复计算。CPU和内存的资源永远比磁盘空间宝贵的多。至于（2)的方式，有点就是灵活，不需要太多的重复清洗，但是性能不如（1）. 建设思路 从需求出发，逆推应用层ADL结构，进而推导出它涉及的主题表IDL表结构，再推导可能涉及的基础表BDL表结构，最后分析所需的数据源取自何处。需求包含“明确”需求和“潜在”需求。 开发步骤 创建ODL、BDL、IDL、ADL层表结构(HQL) 确定数据抽取方案（增量或全量） 编写sqoop脚本将data同步到ODL层 编写ODL-&gt;BDL-&gt;IDL-&gt;ADL层ETL清洗脚本(HQL),注意：清洗的顺序，时间确保上一层的数据稳定，减少对下一层的影响 编写Hue workflow Ooize脚本 打通Kylin、FineBI、Hive关系，实现数据可视化、可导出目标,将稳定后所有脚本WIKI上保存一份 其他相关的请参照原博客 作者：水星有鱼链接：https://www.jianshu.com/p/f941967aeee8","tags":[{"name":"数据仓库","slug":"数据仓库","permalink":"https://wxzhongwang.github.io/tags/数据仓库/"}]},{"title":"SparkStreaming和Storm","date":"2019-01-13T03:33:19.819Z","path":"2019/01/13/SparkStreaming和Storm的区别/","text":"SparkStreaming和StormStorm和Spark Streaming都是分布式流处理的开源框架，但是它们之间还是有一些区别的，这里将进行比较并指出它们的重要的区别。 处理模型以及延迟虽然这两个框架都提供可扩展性(Scalability)和可容错性(Fault Tolerance),但是它们的处理模型从根本上说是不一样的。Storm处理的是每次传入的一个事件，而Spark Streaming是处理某个时间段窗口内的事件流。因此，Storm处理一个事件可以达到亚秒级的延迟，而Spark Streaming则有秒级的延迟。 容错和数据保证在容错数据保证方面的权衡方面，Spark Streaming提供了更好的支持容错状态计算。在Storm中，当每条单独的记录通过系统时必须被跟踪，所以Storm能够至少保证每条记录将被处理一次，但是在从错误中恢复过来时候允许出现重复记录，这意味着可变状态可能不正确地被更新两次。而Spark Streaming只需要在批处理级别对记录进行跟踪处理，因此可以有效地保证每条记录将完全被处理一次，即便一个节点发生故障。虽然Storm的 Trident library库也提供了完全一次处理的功能。但是它依赖于事务更新状态，而这个过程是很慢的，并且通常必须由用户实现。 简而言之,如果你需要亚秒级的延迟，Storm是一个不错的选择，而且没有数据丢失。如果你需要有状态的计算，而且要完全保证每个事件只被处理一次，Spark Streaming则更好。Spark Streaming编程逻辑也可能更容易，因为它类似于批处理程序，特别是在你使用批次(尽管是很小的)时。 实现和编程APIStorm主要是由Clojure语言实现，SparkStreaming是由Scala实现。如果你想看看这两个框架是如何实现的或者你想自定义一些东西你就得记住这一点。Storm是由BackType和Twitter开发，而Spark Streaming是在UC Berkeley开发的。 Storm提供了Java API，同时也支持其他语言的API。SparkStreaming支持Scala和Java语言(其实也支持Python)。另外SparkStreaming的一个很棒的特性就是它是在Spark框架上运行的。这样你就可以想使用其他批处理代码一样来写SparkStreaming程序，或者是在Spark中交互查询。这就减少了单独编写流批量处理程序和历史数据处理程序。 生产支持Storm已经出现好多年了，而且自从2011年开始就在Twitter内部生产环境中使用，还有其他一些公司。而Spark Streaming是一个新的项目，并且在2013年仅仅被Sharethrough使用(据作者了解)。 Storm是 Hortonworks Hadoop数据平台中流处理的解决方案，而Spark Streaming出现在 MapR的分布式平台和Cloudera的企业数据平台中。除此之外，Databricks是为Spark提供技术支持的公司，包括了Spark Streaming。 集群管理集成尽管两个系统都运行在它们自己的集群上，Storm也能运行在Mesos，而SparkStreaming能运行在YARN 和 Mesos上。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"Hadoop Hive Hbase 简单区别及应用场景","date":"2019-01-13T03:30:12.445Z","path":"2019/01/13/Hadoop Hive Hbase 简单区别及应用场景/","text":"Hadoop Hive Hbase 简单区别及应用场景Hadoop它是一个分布式计算+分布式文件系统，前者其实就是MapReduce，后者是HDFS。后者可以独立运行，前者可以选择性使用，也可以不使用。 Hive通俗的说是一个数据仓库，仓库中的数据是被HDFS管理的数据文件，它支持类似sql语句的功能，你可以通过该语句完成分布式环境下的计算功能，Hive会把语句转换成MapReduce，然后交给Hadoop执行。这里的计算，仅限于查找和分析，而不是更新、增加和删除。它的优势是对历史数据进行处理，用时下流行的说法是离线计算，因为它的底层是MapReduce，MapReduce在实时计算上性能很差。它的做法是把数据文件加载进来作为一个Hive表（或者外部表），让你觉得你的sql操作的是传统的表。 HBase通俗的说，HBase的作用类似于数据库，传统数据库管理的是集中的本地数据文件，而HBase基于Hdfs实现对分布式数据文件的管理，比如增删改查。也就是说，HBase只是利用Hadoop的Hdfs帮助其管理数据的持久化文件（HFile），它跟MapReduce没任何关系。HBase的优势在于实时计算，所有实时数据都直接存入Hbase中，客户端通过API直接访问Hbase，实现实时计算。由于它使用的是nosql，或者说是列式结构，从而提高了查找性能，使其能运用于大数据场景，这是它跟MapReduce的区别。 总结： Hadoop是Hive和HBase的基础，Hive依赖Hadoop HBase仅依赖Hadoop的Hdfs模块。 Hive适用于离线数据的分析，操作的是通用格式的（如通用的日志文件）、被Hadoop管理的数据文件，它支持类sql，比编写MapReduce的java代码来的更加方便，它的定位是数据仓库，存储和分析历史数据 Hbase适用于实时计算，采用列式结构的nosql，操作的是自己生成的特殊格式的HFile、被hadoop管理的数据文件，它的定位是数据库，或者叫DBMS 最后补充一下：Hive可以直接操作Hdfs中的文件作为它的表的数据，也可以使HBase数据库作为它的表","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"Hadoop技术体系","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/大数据相关技术（Hadoop体系）/","text":"Hadoop 里面包括几个组件HDFS、MapReduce、YARN和ZooKeeper等一系列技术，HDFS是存储数据的地方就像我们电脑的硬盘一样文件都存储在这个上面，MapReduce是对数据进行处理计算的，YARN是体现Hadoop平台概念的重要组件，有了它大数据生态体系的其它软件就能在hadoop上运行了，这样能更好的利用HDFS大存储的优势和节省更多的资源比如我们就不用再单独建一个spark的集群了，让它直接跑在现有的hadoop yarn上面就可以了。ZooKeeper本身是一个非常牢靠的记事本，用于记录一些概要信息。Hadoop依靠这个记事本来记录当前哪些节点正在用，哪些已掉线，哪些是备用等，以此来管理机群。 Hadoop技术体系HadoopHadoop 里面包括几个组件HDFS、MapReduce、YARN和ZooKeeper等一系列技术，HDFS是存储数据的地方就像我们电脑的硬盘一样文件都存储在这个上面，MapReduce是对数据进行处理计算的，YARN是体现Hadoop平台概念的重要组件，有了它大数据生态体系的其它软件就能在hadoop上运行了，这样能更好的利用HDFS大存储的优势和节省更多的资源比如我们就不用再单独建一个spark的集群了，让它直接跑在现有的hadoop yarn上面就可以了。ZooKeeper本身是一个非常牢靠的记事本，用于记录一些概要信息。Hadoop依靠这个记事本来记录当前哪些节点正在用，哪些已掉线，哪些是备用等，以此来管理机群。 HDFSHadoop Distributed File System，Hadoop 分布式文件系统高度容错性的系统，适合部署在廉价的机器上，HDFS能提供高吞吐量的数据访问，适合那些有着超大数据集（large data set）的应用程序。 MapReduceMapreduce是一个计算框架，一个处理分布式海量数据的软件框架及计算集群。 Map （映射） Reduce (简化)举个例子：假设你的手机通话信息保存在一个HDFS的文件callList.txt中，你想找到你与同事A的所有通话记录并排序。因为HDFS会把callLst.txt分成几块分别存，比如说5块，那么对应的Map过程就是找到这5块所在的5个节点，让它们分别找自己存的那块中关于同事A的通话记录，对应的Reduce过程就是把5个节点过滤后的通话记录合并在一块并按时间排序。MapReduce的计算模型通常把HDFS作为数据来源，很少会用到其它数据来源比如HBase。 Hbase这是Hadoop生态体系中的NOSQL数据库，他的数据是按照key和value的形式存储的并且key是唯一的，所以它能用来做数据的排重，它与MYSQL相比能存储的数据量大很多。所以他常被用于大数据处理完成之后的存储目的地。 HDFS和HBase是依靠外存（即硬盘）的分布式文件存储实现和分布式表存储实现。HDFS是一个分布式的“云存储”文件系统，它会把一个文件分块并分别保存，取用时分别再取出、合并。重要的是，这些分块通常会在3个节点（即集群内的服务器）上各有1个备份，因此即使出现少数节点的失效（如硬盘损坏、掉电等），文件也不会失效。如果说HDFS是文件级别的存储，那HBase则是表级别的存储。HBase是表模型，但比SQL数据库的表要简单的多，没有连接、聚集等功能。HBase的表是物理存储到HDFS的，比如把一个表分成4个HDFS文件并存储。由于HDFS级会做备份，所以HBase级不再备份。MapReduce则是一个计算模型，而不是存储模型；MapReduce通常与HDFS紧密配合。 HiveHive 是一种底层封装了Hadoop 的数据仓库处理工具，使用类SQL的HiveQL语言实现数据查询，所有Hive 的数据都存储在Hadoop 兼容的文件系统（如HDFS）中。Hive在加载数据过程中不会对数据进行任何的修改，只是将数据移动到HDFS中Hive设定的目录下，++因此，Hive不支持对数据的改写和添加，所有的数据都是在加载的时候确定的++。对于会SQL语法的来说就是神器，它能让你处理大数据变的很简单，不会再费劲的编写MapReduce程序。 Spark它是用来弥补基于MapReduce处理数据速度上的缺点，它的特点是把数据装载到内存中计算而不是去读慢的要死进化还特别慢的硬盘。特别适合做迭代运算，所以算法流们特别稀饭它。它是用scala编写的。Java语言或者Scala都可以操作它，因为它们都是用JVM的。 其他相关技术Sqoop这个是用于把Mysql里的数据导入到Hadoop里的。当然你也可以不用这个，直接把Mysql数据表导出成文件再放到HDFS上也是一样的，当然生产环境中使用要注意Mysql的压力。 Flume apache Flume 是一个从可以收集例如日志，事件等数据资源，并将这些数量庞大的数据从各项数据资源中集中起来存储的工具/服务，或者数集中机制。flume具有高可用，分布式，配置工具，其设计的原理也是基于数据流，如日志数据从各种网站服务器上汇集起来存储到HDFS，HBase等集中存储器中。 一般实时系统，所选用组件如下 数据采集 ：负责从各节点上实时采集数据，选用Flume来实现 数据接入 ：由于采集数据的速度和数据处理的速度不一定同步，因此添加一个消息中间件来作为缓冲，选用apache的kafka 流式计算 ：对采集到的数据进行实时分析，选用apache的storm 数据输出 ：对分析后的结果持久化，暂定用mysql，另一方面是模块化之后，假如当Storm挂掉了之后，数据采集和数据接入还是继续在跑着，数据不会丢失，storm起来之后可以继续进行流式计算； KafkaKafka的整体架构非常简单，是显式分布式架构，producer、broker（kafka）和consumer都可以有多个。Producer，consumer实现Kafka注册的接口，数据从producer发送到broker，broker承担一个中间缓存和分发的作用。broker分发注册到系统中的consumer。broker的作用类似于缓存，即活跃的数据和离线处理系统之间的缓存。客户端和服务器端的通信，是基于简单，高性能，且与编程语言无关的TCP协议。 Kafka是一种分布式的、基于发布/订阅的消息系统。在流式计算中，Kafka一般用来缓存数据，Storm通过消费Kafka的数据进行计算（KAFKA+STORM+REDIS）。 特点： 消息持久化：通过O(1)的磁盘数据结构提供数据的持久化 高吞吐量：每秒百万级的消息读写 分布式：扩展能力强 多客户端支持：java、php、python、c++ …… 实时性：生产者生产的message立即被消费者可见 Kafka是一个分布式消息队列：生产者、消费者的功能。它提供了类似于JMS的特性，但是在设计实 现上完全不同，此外它并不是JMS规范的实现。 Kafka对消息保存时根据Topic进行归类，发送消息者称为Producer,消息接受者称为Consumer 无论是kafka集群，还是producer和consumer都依赖于zookeeper集群保存一些meta信息，来保证系统可用性","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"流处理、批处理、交互式查询","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/流处理、批处理、交互式查询/","text":"我们将大数据处理按处理时间的跨度要求分为以下几类 基于实时数据流的处理，通常的时间跨度在数百毫秒到数秒之间 基于历史数据的交互式查询，通常时间跨度在数十秒到数分钟之间 复杂的批量数据处理，通常的时间跨度在几分钟到数小时之间","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"Hadoop概念","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/Hadoop/","text":"Hadoop是一个开源的框架，可编写和运行分布式应用处理大规模数据，是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式。 Hadoop概念 Hadoop是一个开源的框架，可编写和运行分布式应用处理大规模数据，是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式。 ==不是为了大数据而大数据== Hadoop 是以一种可靠、高效、可伸缩的方式进行处理的。Hadoop 是可靠的，因为它假设计算元素和存储会失败，因此它维护多个工作数据副本，确保能够针对失败的节点重新分布处理。Hadoop 是高效的，因为它以并行的方式工作，通过并行处理加快处理速度。Hadoop 还是可伸缩的，能够处理 PB 级数据。核心Hadoop的核心就是==HDFS==和==MapReduce===，Hadoop旗下有很多经典子项目，比如HBase、Hive等，这些都是基于HDFS和MapReduce发展出来的。要想了解Hadoop，就必须知道HDFS和MapReduce是什么。HDFSHadoop Distributed File System，Hadoop 分布式文件系统高度容错性的系统，适合部署在廉价的机器上，HDFS能提供高吞吐量的数据访问，适合那些有着超大数据集（large data set）的应用程序。MapReduceMapreduce是一个计算框架，一个处理分布式海量数据的软件框架及计算集群。 用处 搜索引擎 - 设计Hadoop的初衷，为了针对大规模的网页快速建立索引） 大数据存储 - 利用Hadoop的分布式存储能力，例如数据备份、数据仓库等。 大数据处理 - 利用Hadoop的分布式处理能力，例如数据挖掘、数据分析等。 科学研究 - Hadoop是一种分布式的开源框架，对于分布式计算有很大程度地参考价值。 优缺点优点==高可靠性==Hadoop按位存储和处理数据的能力值得信赖。 ==高扩展性==Hadoop是在可用的计算机集簇间分配数据并完成计算任务的，这些集簇可以方便地扩展到数以千计的节点中。 ==高效性==Hadoop能够在节点之间动态地移动数据，并保证各个节点的动态平衡，因此处理速度非常快。 ==高容错性==Hadoop能够自动保存数据的多个副本，并且能够自动将失败的任务重新分配。 ==低成本==与一体机、商用数据仓库以及QlikView、Yonghong Z-Suite等数据集市相比，hadoop是开源的，项目的软件成本因此会大大降低。 Hadoop设计对硬件需求比较低，只须运行在低廉的商用硬件集群上，而无需昂贵的高可用性机器上。廉价的商用机也就意味着大型集群中出现节点故障情况的概率非常高。这就要求设计HDFS时要充分考虑数据的可靠性，安全性及高可用性。 缺点==不适合低延迟数据访问== 如果要处理一些用户要求时间比较短的低延迟应用请求，则HDFS不适合。HDFS是为了处理大型数据集分析任务的，主要是为达到高的数据吞吐量而设计的，这就可能要求以高延迟作为代价。 改进策略：对于那些有低延时要求的应用程序，HBase是一个更好的选择。通过上层数据管理项目来尽可能地弥补这个不足。在性能上有了很大的提升，它的口号就是goes real time。使用缓存或多master设计可以降低client的数据请求压力，以减少延时。还有就是对HDFS系统内部的修改，这就得权衡大吞吐量与低延时了，HDFS不是万能的银弹。 ==无法高效存储大量小文件== 因为Namenode把文件系统的元数据放置在内存中，所以文件系统所能容纳的文件数目是由Namenode的内存大小来决定。一般来说，每一个文件、文件夹和Block需要占据150字节左右的空间，所以，如果你有100万个文件，每一个占据一个Block，你就至少需要300MB内存。当前来说，数百万的文件还是可行的，当扩展到数十亿时，对于当前的硬件水平来说就没法实现了。还有一个问题就是，因为Maptask的数量是由splits来决定的，所以用MR处理大量的小文件时，就会产生过多的Maptask，线程管理开销将会增加作业时间。举个例子，处理10000M的文件，若每个split为1M，那就会有10000个Maptasks，会有很大的线程开销；若每个split为100M，则只有100个Maptasks，每个Maptask将会有更多的事情做，而线程的管理开销将减小很多。 ==不支持多用户写入及任意修改文件== 在HDFS的一个文件中只有一个写入者，而且写操作只能在文件末尾完成，即只能执行追加操作。目前HDFS还不支持多个用户对同一文件的写操作，以及在文件任意位置进行修改。","tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://wxzhongwang.github.io/tags/Hadoop/"}]},{"title":"大数据","date":"2019-01-08T06:18:02.000Z","path":"2019/01/08/大数据/","text":"大数据生命周期 基础设施层，涵盖计算资源、内存与存储和网络互联，具体表现为计算节点、集群、机柜和数据中心。 数据存储和管理层，包括文件系统、数据库和类似YARN的资源管理系统。 计算处理层，如hadoop、MapReduce和Spark 在此之上的各种不同计算范式，如批处理、流处理和图计算等，包括衍生出编程模型的计算模型，如BSP、GAS等 数据分析和可视化基于计算处理层。分析包括简单的查询分析、流分析以及更复杂的分析(如机器学习、图计算等)。查询分析多基于表结构和关系函数，流分析基于数据、事件流以及简单的统计分析，而复杂分析则基于更复杂的数据结构与方法，如图、矩阵、迭代计算和线性代数。一般意义的可视化是对分析结果的展示。但是通过交互式可视化，还可以探索性地提问，使分析获得新的线索，形成迭代的分析和可视化。基于大规模数据的实时交互可视化分析以及在这个过程中引入自动化的因素是目前研究的热点。 大数据技术生态大数据的基本处理流程与传统数据处理流程并无太大差异，主要区别在于：由于大数据要处理大量、非结构化的数据，所以在各处理环节中都可以采用并行处理。目前，Hadoop、MapReduce和Spark等分布式处理方式已经成为大数据处理各环节的通用处理方法。 大数据采集与预处理 存储层 预处理层 采集层 在大数据的生命周期中，数据采集处于第一个环节。根据MapReduce产生数据的应用系统分类，大数据的采集主要有4种来源：管理信息系统、Web信息系统、物理信息系统、科学实验系统。对于不同的数据集，可能存在不同的结构和模式，如文件、XML树、关系表等，表现为数据的异构性。对多个异构的数据集，需要做进一步集成处理或整合处理，将来自不同数据集的数据收集、整理、清洗、转换后，生成到一个新的数据集，为后续查询和分析处理提供统一的数据视图。针对管理信息系统中异构数据库集成技术、Web信息系统中的实体识别技术和DeepWeb集成技术、传感器网络数据融合技术已经有很多研究工作，取得了较大的进展，已经推出了多种数据清洗和质量控制工具。 大数据的存储和管理按数据类型的不同，大数据的存储和管理采用不同的技术路线，大致可以分为3类。 第1类主要面对的是大规模的结构化数据。针对这类大数据，通常采用新型数据库集群。它们通过列存储或行列混合存储以及粗粒度索引等技术，结合MPP(MassiveParallelProcessing)架构高效的分布式计算模式，实现对PB量级数据的存储和管理。这类集群具有高性能和高扩展性特点，在企业分析类应用领域已获得广泛应用; 第2类主要面对的是半结构化和非结构化数据。应对这类应用场景，基于Hadoop开源体系的系统平台更为擅长。它们通过对Hadoop生态体系的技术扩展和封装，实现对半结构化和非结构化数据的存储和管理; 第3类面对的是结构化和非结构化混合的大数据，因此采用MPP并行数据库集群与Hadoop集群的混合来实现对百PB量级、EB量级数据的存储和管理。一方面，用MPP来管理计算高质量的结构化数据，提供强大的SQL和OLTP型服务;另一方面，用Hadoop实现对半结构化和非结构化数据的处理，以支持诸如内容检索、深度挖掘与综合分析等新型应用。这类混合模式将是大数据存储和管理未来发展的趋势。","tags":[{"name":"大数据","slug":"大数据","permalink":"https://wxzhongwang.github.io/tags/大数据/"}]},{"title":"消息队列","date":"2018-12-08T03:18:02.000Z","path":"2018/12/08/消息队列/","text":"一、消息队列的基本概念1.1 Broker==Broker== 的概念来自与Apache ActiveMQ，通俗的讲就是消息队列服务器。 1.2 消息生产者和消费者 消息生产者 ==Producer==：发送消息到消息队列。 消息消费者 ==Consumer==：从消息队列接收消息。 1.3 消息模型点对点消息队列模型消息生产者向一个特定的队列发送消息，消息消费者从该队列中接收消息。消息的生产者和消费者可以不同时处于运行状态。每一个成功处理的消息都由消息消费者签收确认（Acknowledge）。 发布订阅消息模型-Topic发布订阅消息模型中，支持向一个特定的主题Topic发布消息，0个或多个订阅者接收来自这个消息主题的消息。在这种模型下，发布者和订阅者彼此不知道对方。实际操作过程中，必须先订阅，再发送消息，而后接收订阅的消息，这个顺序必须保证。 1.4 消息顺序性保证基于Queue消息模型，利用FIFO先进先出的特性，可以保证消息的顺序性。 1.5 消息的ACK确认机制即消息的Ackownledge确认机制：为了保证消息不丢失，消息队列提供了消息Acknowledge机制，即ACK机制，当Consumer确认消息已经消费处理，发送一个ACK给消息队列，此时消息队列便可以删除这个消息了。如果Consumer宕机/关闭，没有发送ACK，消息队列将认为这个消息没有被处理，会将这个消息重新发送给其他的Consumer重新消费处理。 1.6 消息的持久化消息的持久化，对于一些关键的核心业务来说是非常重要的，启用消息持久化后，消息队列宕机重启后，消息可以从持久化存储恢复，消息不丢失，可以继续消费处理。 1.7 消息的同步和异步收发同步消息的收发支持同步收发的方式同步收发场景下，消息生产者和消费者双向应答模式，例如：张三写封信送到邮局中转站，然后李四从中转站获得信，然后在写一份回执信，放到中转站，然后张三去取，当然张三写信的时候就得写明回信地址。消息的接收如果以同步的方式(Pull)进行接收，如果队列中为空，此时接收将处于同步阻塞状态，会一直等待，直到消息的到达。 异步消息的收发同样支持异步方式：异步发送消息，不需要等待消息队列的接收确认。异步接收消息，以Push的方式触发消息消费者接收消息。 1.8 消息的事务支持消息的收发处理支持事务，例如：在任务中心场景中，一次处理可能涉及多个消息的接收、处理，这处于同一个事务范围内，如果一个消息处理失败，事务回滚，消息重新回到队列中。 二、JMS消费服务Java消息服务（Java Message Service，JMS）应用程序接口是一个Java平台中关于面向消息中间件（MOM）的API，用于在两个应用程序之间，或分布式系统中发送消息，进行异步通信。 点对点与发布订阅最初是由JMS定义的。这两种模式主要区别或解决的问题就是发送到队列的消息能否重复消费(多订阅) 。 JMS规范目前支持两种消息模型： 点对点（point to point， queue） 发布/订阅（publish/subscribe，topic） 2.1 点对点：Queue，不可重复消费消息生产者生产消息发送到queue中，然后消息消费者从queue中取出并且消费消息。消息被消费以后，queue中不再有存储，所以消息消费者不可能消费到已经被消费的消息。Queue支持存在多个消费者，但是对一个消息而言，只会有一个消费者可以消费。 P2P模式包含三个角色： 消息队列（Queue） 发送者(Sender) 接收者(Receiver) 每个消息都被发送到一个特定的队列，接收者从队列中获取消息。队列保留着消息，直到他们被消费或超时。 2.2 发布/订阅：Topic，可以重复消费消息生产者（发布）将消息发布到topic中，同时有多个消息消费者（订阅）消费该消息。和点对点方式不同，发布到topic的消息会被所有订阅者消费。 支持订阅组的发布订阅模式：发布订阅模式下，当发布者消息量很大时，显然单个订阅者的处理能力是不足的。实际上现实场景中是多个订阅者节点组成一个订阅组负载均衡消费topic消息即分组订阅，这样订阅者很容易实现消费能力线性扩展。可以看成是一个topic下有多个Queue，每个Queue是点对点的方式，Queue之间是发布订阅方式。 2.3 区别点对点模式生产者发送一条消息到queue，一个queue可以有很多消费者，但是一个消息只能被一个消费者接受，当没有消费者可用时，这个消息会被保存直到有一个可用的消费者，所以Queue实现了一个可靠的负载均衡。 发布订阅模式发布者发送到topic的消息，只有订阅了topic的订阅者才会收到消息。topic实现了发布和订阅，当你发布一个消息，所有订阅这个topic的服务都能得到这个消息，所以从1到N个订阅者都能得到这个消息的拷贝。 三、流行模型对比传统企业型消息队列ActiveMQ遵循了JMS规范，实现了点对点和发布订阅模型，但其他流行的消息队列RabbitMQ、Kafka并没有遵循JMS规范。 3.1 RabbitMQRabbitMQ实现了AQMP协议，AQMP协议定义了消息路由规则和方式。生产端通过路由规则发送消息到不同queue，消费端根据queue名称消费消息。RabbitMQ既支持内存队列也支持持久化队列，消费端为推模型，消费状态和订阅关系由服务端负责维护，消息消费完后立即删除，不保留历史消息。 点对点生产端发送一条消息通过路由投递到Queue，只有一个消费者能消费到。 多订阅当RabbitMQ需要支持多订阅时，发布者发送的消息通过路由同时写到多个Queue，不同订阅组消费不同的Queue。所以支持多订阅时，消息会多个拷贝。 3.2 KafkaKafka只支持消息持久化，消费端为拉模型，消费状态和订阅关系由客户端端负责维护，消息消费完后不会立即删除，会保留历史消息。因此支持多订阅时，消息只会存储一份就可以了。但是可能产生重复消费的情况。","tags":[{"name":"MessageQueue","slug":"MessageQueue","permalink":"https://wxzhongwang.github.io/tags/MessageQueue/"}]},{"title":"你好，世界","date":"2018-08-31T09:54:54.000Z","path":"2018/08/31/hello-world/","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","tags":[{"name":"前端","slug":"前端","permalink":"https://wxzhongwang.github.io/tags/前端/"},{"name":"博客系统","slug":"博客系统","permalink":"https://wxzhongwang.github.io/tags/博客系统/"}]},{"title":"sass sass-loader","date":"2018-06-12T08:45:02.000Z","path":"2018/06/12/windows下安装node-sass和sass-loader失败/","text":"window下无法安装sass sass-loader node-sass安装失败的原因是网络限制导致无法下载.node文件 推荐方法：使用淘宝镜像 1npm set sass_binary_site=https://npm.taobao.org/mirrors/node-sass/ or 12npm install -g cnpm --registry=https://registry.npm.taobao.orgcnpm install node-sass sass-loader -S 其他翻墙、手动导入文件的方式不推荐。","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"}]},{"title":"Mock.js","date":"2018-04-08T05:22:02.000Z","path":"2018/04/08/Mockjs/","text":"目前的大部分公司的项目都是采用的前后端分离, 后端接口的开发和前端人员是同时进行的. 那么这个时候就会存在一个问题, 在页面需要使用大量数据进行渲染生成前, 后端开发人员的接口也许并没有写完, 作为前端的我们也就没有办法获取数据. 所以 前端工程师就需要自己按照接口文档模拟后端人员提供的数据, 以此进行页面的开发.这个时候, Mock.js的作用就体现出来了, 在数据量较大的情况下, 我们不用一个一个的编写数据, 只需要根据接口文档将数据的格式填入,Mock.js就能够自动的按需生成大量的模拟数据. 且Mock.js提供了大量的数据类型, 包括文本, 数字, 布尔值, 日期, 邮箱, 链接, 图片, 颜色等. Mock.jsMockjs是什么?目前的大部分公司的项目都是采用的前后端分离, 后端接口的开发和前端人员是同时进行的. 那么这个时候就会存在一个问题, 在页面需要使用大量数据进行渲染生成前, 后端开发人员的接口也许并没有写完, 作为前端的我们也就没有办法获取数据. 所以 前端工程师就需要自己按照接口文档模拟后端人员提供的数据, 以此进行页面的开发.这个时候, Mock.js的作用就体现出来了, 在数据量较大的情况下, 我们不用一个一个的编写数据, 只需要根据接口文档将数据的格式填入,Mock.js就能够自动的按需生成大量的模拟数据. 且Mock.js提供了大量的数据类型, 包括文本, 数字, 布尔值, 日期, 邮箱, 链接, 图片, 颜色等. 安装Mockjs123npm install mockjs -S or npm install mockjs -D 引用MockjsMock.js暴露了一个全局的Mock对象, 我们只需要将Mock对象引入到文件中, 调用Mock对象的方法即可 CommonJS的引入方式 12345678910//CommonJS引入let Mock = require('mockjs) //调用Mock.mock()方法模拟数据let data = Mock.mock(&#123;'list|1-10': [&#123; 'id|+1': 1&#125;]&#125;);console.log(data); ES6的引入方式 123456789//ES6的引入方式import Mock from 'mockjs' let data = Mock.mock(&#123;'list|1-10': [&#123; 'id|+1': 1&#125;]&#125;);console.log(data); 简单用法Mock对象提供了4个方法, 分别是 Mock.mock() Mock.setup() Mock.valid Mock.toJSONSchema() 以及一个工具库 Mock.Random. 其中我们经常使用到的就是Mock.mock()和Mock.Random.","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"}]},{"title":"Linux常用命令","date":"2018-03-06T08:18:02.000Z","path":"2018/03/06/Linux 常用命令/","text":"Linux常用命令 #Linux 常用命令 cd ==cd /== 进入主目录 ==cd ~== 进入Home目录 ==cd -== 进入上一次工作中路径 ls ==ls -a== 列出所有 ==ls -r== 反序排列 ==ls -t== 以文件修改时间排列 ==ls -l== 将文件名,文件所有者，文件大小信息详细信息列出来 pwd ==pwd== 展示当前工作目录路径 mkdir创建文件夹可用选项： ==mkdir -m==: 对新建目录设置存取权限,也可以用chmod命令设置; ==mkdir -p==: 可以是一个路径名称。此时若路径中的某些目录尚不存在,加上此选项后,系统将自动建立那 些尚不在的目录,即一次可以建立多个目录; rm删除文件，删除一个目录中的一个或多个文件或目录，如果没有使用- r选项，则rm不会删除目录 rmdir从一个目录中删除一个或多个子目录项，删除某目录时也必须具有对其父目录的写权限。 mv移动文件或修改文件名，根据第二参数类型（如目录，则移动文件；如为文件则重命令该文件）。 cp将源文件复制至目标文件，或将多个源文件复制至目标目录。 free显示系统内存使用情况，包括物理内存、交互区内存(swap)和内核缓冲区内存 -b 以Byte显示内存使用情况 -k 以kb为单位显示内存使用情况 -m 以mb为单位显示内存使用情况 -g 以gb为单位显示内存使用情况 -s &lt;间隔秒数&gt; 持续显示内存 -t 显示内存使用总合 cat cat主要有三大功能： 一次显示整个文件:cat filename 从键盘创建一个文件:cat &gt; filename 只能创建新文件,不能编辑已有文件. 将几个文件合并为一个文件:cat file1 file2 &gt; file -b对非空输出行号 -n输出所有行号","tags":[{"name":"Linux","slug":"Linux","permalink":"https://wxzhongwang.github.io/tags/Linux/"}]},{"title":"HttpStatusCode","date":"2018-02-08T04:18:45.000Z","path":"2018/02/08/HttpStatusCode/","text":"HttpStatusCode12345678 /* 1xx：相关信息 2xx：操作成功 3xx：重定向 4xx：客户端错误 5xx：服务器错误*/ 字段 状态码 说明 Continue 100 指示客户端可能继续其请求。 SwitchingProtocols 100 指示正在更改协议版本或协议。 OK 200 指示请求成功，且请求的信息包含在响应中。 Created 201 指示请求导致在响应被发送前创建新资源。 Accepted 202 指示请求已被接受做进一步处理。 NonAuthoritativeInformation 202 指示返回的元信息来自缓存副本而不是原始服务器，因此可能不正确。 NoContent 204 指示请求成功，指示已成功处理请求并且响应已被设定为无内容。 ResetContent 205 指示客户端应重置（或重新加载）当前资源。 PartialContent 206 指示响应是包括字节范围的 GET 请求所请求的部分响应。 MultipleChoices 300 指示请求的信息有多种表示形式，默认操作是将此状态视为重定向，并遵循与此响应关联的 Location 头的内容。 Ambiguous 300 指示请求的信息有多种表示形式。默认操作是将此状态视为重定向，并遵循与此响应关联的 Location 头的内容。 MovedPermanently 301 指示请求的信息已移到 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。 Moved 301 指示请求的信息已移到 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求将使用 GET 方法。 Found 302 指示请求的信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求将使用 GET 方法。 Redirect 302 指示请求的信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求将使用 GET 方法。 SeeOther 303 作为 POST 的结果，SeeOther 将客户端自动重定向到 Location 头中指定的 URI。用 GET 生成对 Location 头所指定的资源的请求。 RedirectMethod 303 作为 POST 的结果，RedirectMethod 将客户端自动重定向到 Location 头中指定的 URI。用 GET 生成对 Location 头所指定的资源的请求。 NotModified 304 指示客户端的缓存副本是最新的。未传输此资源的内容。 UseProxy 305 指示请求应使用位于 Location 头中指定的 URI 的代理服务器。 Unused 306 是未完全指定的 HTTP/1.1 规范的建议扩展。 TemporaryRedirect 307 指示请求信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求还将使用 POST 方法。 RedirectKeepVerb 307 指示请求信息位于 Location 头中指定的 URI 处。接收到此状态时的默认操作为遵循与响应关联的 Location 头。原始请求方法为 POST 时，重定向的请求还将使用 POST 方法。 BadRequest 400 指示服务器未能识别请求。如果没有其他适用的错误，或者如果不知道准确的错误或错误没有自己的错误代码，则发送 BadRequest。 Unauthorized 401 指示请求的资源要求身份验证。WWW-Authenticate头包含如何执行身份验证的详细信息。 PaymentRequired 402 保留 PaymentRequired 以供将来使用。 Forbidden 403 指示服务器拒绝满足请求。 NotFound 404 指示请求的资源不在服务器上。 MethodNotAllowed 405 指示请求的资源上不允许请求方法（POST 或 GET）。 NotAcceptable 406 指示客户端已用 Accept 头指示将不接受资源的任何可用表示形式。 ProxyAuthenticationRequired 407 指示请求的代理要求身份验证。Proxy-authenticate 头包含如何执行身份验证的详细信息。 RequestTimeout 408 指示客户端没有在服务器期望请求的时间内发送请求。 Conflict 409 指示由于服务器上的冲突而未能执行请求。 Gone 410 指示请求的资源不再可用。 LengthRequired 411 指示缺少必需的 Content-length 头。 PreconditionFailed 412 指示为此请求设置的条件失败，且无法执行此请求。条件是用条件请求标头（如 If-Match、If-None-Match 或 If-Unmodified-Since）设置的。 RequestEntityTooLarge 413 指示请求太大，服务器无法处理。 RequestUriTooLong 414 指示 URI 太长。 UnsupportedMediaType 415 指示请求是不支持的类型。 RequestedRangeNotSatisfiable 416 RequestedRangeNotSatisfiable指示无法返回从资源请求的数据范围，因为范围的开头在资源的开头之前，或因为范围的结尾在资源的结尾之后。 ExpectationFailed 417 指示服务器未能符合 Expect 头中给定的预期值。 UpgradeRequired 426 客户端应当切换到TLS/1.0 InternalServerError 500 指示服务器上发生了一般错误。 NotImplemented 501 指示服务器不支持请求的函数。 BadGateway 502 指示中间代理服务器从另一代理或原始服务器接收到错误响应。 ServiceUnavailable 503 指示服务器暂时不可用，通常是由于过多加载或维护。 GatewayTimeout 504 指示中间代理服务器在等待来自另一个代理或原始服务器的响应时已超时。 HttpVersionNotSupported 505 指示服务器不支持请求的 HTTP 版本。","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"},{"name":"前端","slug":"前端","permalink":"https://wxzhongwang.github.io/tags/前端/"}]},{"title":"Greenplum 从入门到放弃（四）","date":"2018-02-04T02:10:00.000Z","path":"2018/02/04/Greenplum 从入门到放弃（四）/","text":"Greenplum 从入门到放弃（四）PostgreSQL与Greenplum的关系PostgreSQLPostgreSQL是一种非常先进的对象–关系型数据库管理系统（ORDBMS），是目前功能最强大，特性最丰富和技术最先进的自由软件数据库系统之一，其某些特性甚至连商业数据库都不具备。 PostgreSQL的特点可以说是数不胜数，称其为最先进的开源软件数据库当之无愧，支持绝大部分的主流数据库特性，主要体现在如下几方面： 函数/存储过程 PostgreSQL对非常丰富的过程类语言提供支持，可以编写自定义函数/存储过程 内置的plpgsql，一种类似Oracle的PLsql的语言 支持的脚本语言有：PL/Lua、PL/LOLCODE、PL/Perl、PL/HP、PL/Python、PL/Ruby、PL/sh、PL/Tcl和PL/Scheme。、 编译语言有C、C++和JAVA。 ·统计语言PL/R 索引 PostgreSQL支持用户定义的索引访问方法，并且内置了Btree、哈希和GiST索引。PostgreSQL中的索引有下面几个特点： 可以从后向前扫描 可以创建表达式索引 部分索引 触发器 触发器是由SQL查询的动作触发的事件。比如，一个INSERT查询可能激活一个检查输入值是否有效的触发器。大多数触发器都只对INSERT或者UPDATE查询有效。PostgreSQL完全支持触发器，可以附着在表上，但是不能在视图上。不过视图可以有规则。多个触发器是按照字母顺序触发的。我们还可以用其他过程语言书写触发器函数，不仅仅PL/PgSQL。 并发管理（MVCC） PostgreSQL的并发管理使用的是一种叫做“MVCC”（多版本并发机制）的机制，这种机制实际上就是现在在众多所谓的编程语言中极其火爆的“Lock Free”，其本质是通过类似科幻世界的时空穿梭的原理，给予每个用户一个自己的“时空”，然后通过原子的“时空”控制来控制时间基线，并以此控制并发更改的可见区域，从而实现近乎无锁的并发，而同时还能在很大程度上保证数据库的ACID特性。 规则（RULE） 规则允许我们对由一个查询生成的查询树进行改写。 数据类型 PostgreSQL支持非常广泛的数据类型，包括： 任意精度的数值类型； 无限长度的文本类型； 几何原语； IPv4和IPv6类型； CIDR块和MAC地址； 数组。 用户还可以创建自己的类型，并且可以利用GiST框架把这些类型做成完全可索引的，比如来自PostGIS的地理信息系统（GIS）的数据类型。 用户定义对象 因为PostgreSQL使用一种基于系统表的可扩展的结构设计，所以PostgreSQL内部的几乎所有对象都可以由用户定义，这些对象包括： 索引； 操作符（内部操作符可以被覆盖）； 聚集函数； 域； 类型转换； 编码转换。 继承 PostgreSQL的表是可以相互继承的。一个表可以有父表，父表的结构变化会导致子表的结构变化，而对子表的插入和数据更新等也会反映到父表中。 其他特性与扩展 二进制和文本大对象存储； 在线备份； TOAST（The Oversized-Attribute Storage Technique）用于透明地在独立的地方保存大的数据库属性，当数据超过一定大小的时候，会自动进行压缩以节省空间； 正则表达式。 此外PostgreSQL还有大量的附加模块和扩展版本，比如，多种不同的主从/主主复制方案: Slony-I； pgcluster； Mammoth replicator； Bucardo。","tags":[{"name":"Greenplum","slug":"Greenplum","permalink":"https://wxzhongwang.github.io/tags/Greenplum/"}]},{"title":"Greenplum 从入门到放弃（三）","date":"2018-02-03T02:10:00.000Z","path":"2018/02/03/Greenplum 从入门到放弃（三）/","text":"Greenplum 从入门到放弃（三）master 和 segment关系Master和Segment其实都是一个单独的PostgreSQL数据库。每一个都有自己单独的一套元数据字典，在这里，Master节点一般也叫主节点，Segment也叫做数据节点。Segment节点与Master节点的通信，通过千兆（或万兆）网卡组成的内部连接（InterConnect），在同一台数据节点机器上可以放多个Segment，不同的Segment节点会被赋予不同的端口，同时，Segment之间也不断地进行着交互。为了实现高可用，每个Segment都有对应的备节点（Mirror Segment），分别存在于不同的机器上。 Client一般只能与Master节点进行交互，Client将SQL发给Master，然后Master对SQL进行分析后，再将其分配给所有的Segment进行操作，并且将汇总结果返回给客户端。 数据库存储对于数据库来说，在性能上磁盘IO很容易成为瓶颈，由于数据库的特性，每一个SQL基本都是对全表数据进行分析，每次处理的数据量非常大，数据基本上都是没有缓存的（数据字典除外），极度消耗IO资源（全表扫描主要都是顺序IO），所以Greenplum对存储的要求比较高。在文件系统的选择上，在Linux下建议使用XFS，在Solaris下建议使用ZFS，对于Raid根据需求选择硬Raid或软Raid，如果需要更大的空间，建议使用Raid5，如果对性能有更高的要求，可以选择Raid 1+0。 网络在确定机器配置的时候，要保证所有机器的网络都是通的，并且每台机器的防火墙都是关闭的，避免存在网络不通的问题。","tags":[{"name":"Greenplum","slug":"Greenplum","permalink":"https://wxzhongwang.github.io/tags/Greenplum/"}]},{"title":"Greenplum 从入门到放弃（二）","date":"2018-02-02T02:10:00.000Z","path":"2018/02/02/Greenplum 从入门到放弃（二）/","text":"Greenplum 从入门到放弃（二）OLTP与OLAP数据库系统一般分为两种类型，一种是面向前台应用的，应用比较简单，但是重吞吐和高并发的OLTP类型；一种是重计算的，对大数据集进行统计分析的OLAP类型。Greenplum属于后者。 OLTP（On-Line TransactionProcessing，联机事务处理）系统也称为生产系统，它是事件驱动的、面向应用的，比如电子商务网站的交易系统就是一个典型的OLTP系统。OLTP的基本特点是： 数据在系统中产生 基于交易的处理系统（Transaction-Based） 每次交易牵涉的数据量很小 对响应时间要求非常高 用户数量非常庞大，主要是操作人员 数据库的各种操作主要基于索引进行 OLAP（On-Line Analytical Processing，联机分析处理）是基于数据仓库的信息分析处理过程，是数据仓库的用户接口部分。OLAP系统是跨部门的、面向主题的，其基本特点是： 本身不产生数据，其基础数据来源于生产系统中的操作数据（OperationalData） 基于查询的分析系统 复杂查询经常使用多表联结、全表扫描等，牵涉的数据量往往十分庞大 响应时间与具体查询有很大关系 用户数量相对较小，其用户主要是业务人员与管理人员 由于业务问题不固定，数据库的各种操作不能完全基于索引进行","tags":[{"name":"Greenplum","slug":"Greenplum","permalink":"https://wxzhongwang.github.io/tags/Greenplum/"}]},{"title":"Greenplum 从入门到放弃（一）","date":"2018-02-01T02:10:00.000Z","path":"2018/02/01/Greenplum 从入门到放弃（一）/","text":"Greenplum 从入门到放弃（一） Greenplum的性能在数据量为TB级别时表现非常优秀，单机性能相比Hadoop要快好几倍 Greenplum是基于PostgreSQL的一个完善的数据库，在功能和语法上都要比Hadoop上的SQL引擎Hive好用很多，对于普通用户来说更加容易上手。 Greenplum有着完善的工具，相比Hive，整个体系都比较完善，不需要像Hive一样花太多的时间和精力进行改造，非常适合作为一些大型的数据仓库解决方案。 Greenplum能够方便地与Hadoop进行结合，可直接把数据写在Hadoop上，还可以直接在数据库上写MapReduce任务，并且配置简单。","tags":[{"name":"Greenplum","slug":"Greenplum","permalink":"https://wxzhongwang.github.io/tags/Greenplum/"}]},{"title":"Git使用中的问题","date":"2018-01-08T04:20:02.000Z","path":"2018/01/08/Git使用中的问题/","text":"git push失败 fatal: Could not read from remote repository 阐述问题git push失败 fatal: Could not read from remote repository. 因为仓库地址不对。更改地址就可以push了。 问题原因12$ git remote -v$ git remote set-url origin XXX 服务器上的 Git - 生成 SSH 公钥生成 SSH 公钥大多数 Git 服务器都会选择使用 SSH 公钥来进行授权。系统中的每个用户都必须提供一个公钥用于授权，没有的话就要生成一个。生成公钥的过程在所有操作系统上都差不多。首先先确认一下是否已经有一个公钥了。SSH 公钥默认储存在账户的主目录下的 ~/.ssh 目录。若想在github中使用的话需要将公钥复制到github&gt;setting&gt;SSH and GPG keys中添加ssh keys。 1234生成钥匙$ ssh-keygen查看公钥$cat ~/.ssh/id_rsa.pub","tags":[{"name":"Git","slug":"Git","permalink":"https://wxzhongwang.github.io/tags/Git/"}]},{"title":"优秀的开源项目","date":"2017-12-08T02:18:02.000Z","path":"2017/12/08/优秀的开源项目及优秀文章地址/","text":"此部分总结平时工作中积累的项目或者见过的优秀开源项目的总结 优秀的开源项目此部分总结平时工作中积累的项目或者见过的优秀开源项目的总结 前端相关NET相关NodeJS相关NodeJS 中文社区开源官网地址: https://cnodejs.org/ 项目开源地址： https://github.com/cnodejs/nodeclub/ JAVA相关数据库相关其他架构相关优秀文章地址此部分总结平时工作中积累的优秀文章或者博客 前端、JAVA、Python https://www.jqhtml.com/ Nginx:详细文档: http://tengine.taobao.org/book/index.html Jekins:教程http://blog.51cto.com/12832314/2140304","tags":[{"name":"Web","slug":"Web","permalink":"https://wxzhongwang.github.io/tags/Web/"}]},{"title":"Redis简介","date":"2017-09-12T10:33:59.000Z","path":"2017/09/12/Redis简介/","text":"Redis是一个开源的，使用C语言编写，面向“键/值”对类型数据的分布式NoSQL数据库系统，特点是高性能，持久存储，适应高并发的应用场景。Redis纯粹为应用而产生，它是一个高性能的key-value数据库,并且提供了多种语言的API，性能测试结果表示SET操作每秒钟可达110000次，GET操作每秒81000次（当然不同的服务器配置性能不同）。 RedisRedis是一个开源的，使用C语言编写，面向“键/值”对类型数据的分布式NoSQL数据库系统，特点是高性能，持久存储，适应高并发的应用场景。Redis纯粹为应用而产生，它是一个高性能的key-value数据库,并且提供了多种语言的API，性能测试结果表示SET操作每秒钟可达110000次，GET操作每秒81000次（当然不同的服务器配置性能不同）。 Redis目前提供五种数据类型： string(字符串), list（链表）, Hash（哈希）, set（集合）, zset(sorted set) （有序集合） Redis开发维护很活跃，虽然它是一个Key-Value数据库存储系统，但它本身支持MQ功能，所以完全可以当做一个轻量级的队列服务来使用。 Redis可以做消息队列？ 首先，redis设计用来做缓存的，但是由于它自身的某种特性使得它可以用来做消息队列，它有几个阻塞式的API可以使用，正是这些阻塞式的API让其有能力做消息队列；另外，做消息队列的其他特性例如FIFO（先入先出）也很容易实现，只需要一个list对象从头取数据，从尾部塞数据即可；redis能做消息队列还得益于其list对象blpop brpop接口以及Pub/Sub（发布/订阅）的某些接口，它们都是阻塞版的，所以可以用来做消息队列。 对于RabbitMQ和Redis的入队和出队操作，各执行100万次，每10万次记录一次执行时间。测试数据分为128Bytes、512Bytes、1K和10K四个不同大小的数据。实验表明： 入队时，当数据比较小时Redis的性能要高于RabbitMQ，而如果数据大小超过了10K，Redis则慢的无法忍受；出队时，无论数据大小，Redis都表现出非常好的性能，而RabbitMQ的出队性能则远低于Redis。","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"},{"name":"消息队列","slug":"消息队列","permalink":"https://wxzhongwang.github.io/tags/消息队列/"}]},{"title":"Redis和Memcached比较","date":"2017-09-12T10:33:59.000Z","path":"2017/09/12/Redis和Memcached比较/","text":"Redis和Memcache都是将数据存放在内存中，都是内存数据库。本文介绍两者的区别。 Redis和Memcached比较 Memcached是多线程，而Redis使用单线程. Memcached使用预分配的内存池的方式，Redis使用现场申请内存的方式来存储数据，并且可以配置虚拟内存。 Redis可以实现持久化，主从复制，实现故障恢复。 Memcached只是简单的key与value,但是Redis支持数据类型比较多。 Redis的存储分为内存存储、磁盘存储 .从这一点，也说明了Redis与Memcached是有区别的。Redis 与Memcached一样，为了保证效率，数据都是缓存在内存中。区别的是redis会周期性的把更新的数据写入磁盘或者把修改 操作写入追加的记录文件，并且在此基础上实现了master-slave(主从)同步。 Redis有两种存储方式(默认:snapshot) snapshot 实现方法是定时将内存的快照(snapshot)持久化到硬盘，这种方法缺点是持久化之后如果出现crash则会丢失一段数据。因此在完美主义者的推动下作者增加了aof方式。 aof 即append only mode，在写入内存数据的同时将操作命令保存到日志文件，在一个并发更改上万的系统中，命令日志是一个非常庞大的数据，管理维护成本非常高，恢复重建时间会非常长，这样导致失去aof高可用性本意。另外更重要的是Redis是一个内存数据结构模型，所有的优势都是建立在对内存复杂数据结构高效的原子操作","tags":[{"name":"Redis","slug":"Redis","permalink":"https://wxzhongwang.github.io/tags/Redis/"},{"name":"NOSQL","slug":"NOSQL","permalink":"https://wxzhongwang.github.io/tags/NOSQL/"}]}]